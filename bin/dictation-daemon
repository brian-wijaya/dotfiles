#!/home/bw/.local/share/dictation-venv/bin/python3
"""Dictation daemon: keeps faster-whisper model loaded, transcribes on demand via Unix socket."""

import json
import os
import signal
import socket
import sys

SOCKET_PATH = "/tmp/dictation.sock"
MODEL_PATH = os.path.expanduser(
    "~/.cache/huggingface/hub/models--deepdml--faster-whisper-large-v3-turbo-ct2/"
    "snapshots/44cbbd1adefe7387c83df88963a6d9ac4c9adea5"
)
DEVICE = "cuda"
COMPUTE_TYPE = "float16"


def load_model():
    from faster_whisper import WhisperModel

    print(f"Loading model from {MODEL_PATH} on {DEVICE} ({COMPUTE_TYPE})...", flush=True)
    model = WhisperModel(
        MODEL_PATH,
        device=DEVICE,
        compute_type=COMPUTE_TYPE,
        local_files_only=True,
    )
    print("Model loaded.", flush=True)
    return model


def transcribe(model, audio_path):
    segments, info = model.transcribe(
        audio_path,
        language="en",
        beam_size=5,
        vad_filter=True,
        vad_parameters=dict(
            min_silence_duration_ms=300,
        ),
    )
    text = " ".join(seg.text.strip() for seg in segments)
    return text.strip()


def run_server(model):
    if os.path.exists(SOCKET_PATH):
        os.unlink(SOCKET_PATH)

    sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
    sock.bind(SOCKET_PATH)
    os.chmod(SOCKET_PATH, 0o600)
    sock.listen(1)

    def cleanup(signum, frame):
        sock.close()
        if os.path.exists(SOCKET_PATH):
            os.unlink(SOCKET_PATH)
        sys.exit(0)

    signal.signal(signal.SIGTERM, cleanup)
    signal.signal(signal.SIGINT, cleanup)

    print(f"Listening on {SOCKET_PATH}", flush=True)

    while True:
        conn, _ = sock.accept()
        try:
            data = conn.recv(4096).decode().strip()
            if not data or not os.path.isfile(data):
                conn.sendall(b"")
                continue

            text = transcribe(model, data)
            conn.sendall(text.encode())
        except Exception as e:
            print(f"Error: {e}", file=sys.stderr, flush=True)
            conn.sendall(b"")
        finally:
            conn.close()


if __name__ == "__main__":
    model = load_model()
    run_server(model)
