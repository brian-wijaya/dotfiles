vocabulary:
  layer-1: Reactive versioning — fanotify daemon captures every file close-after-write, stores as content-addressed chunks
  layer-2: Periodic snapshots — hardlink filesystem snapshots every 60 seconds, 24-hour retention, no daemon dependency
  layer-3: Off-site archive — rsync push to NAS over SSH, ZFS-encrypted at rest, ZFS-scrubbed for integrity, survives local destruction
  fanotify: Linux kernel facility for filesystem event notification; no watch limits, kernel-guaranteed delivery
  panama-ffi: JDK 25 Foreign Function & Memory API; calls fanotify syscall directly from Java without JNI
  fastcdc: Content-Defined Chunking algorithm using gear hash; produces 8-32KB variable-length chunks at content boundaries
  content-addressed: Chunks stored by SHA-256 hash of content; identical content produces identical hash; deduplication is automatic
  hardlink-snapshot: cp -al creates directory tree where files are hardlinks to originals; O(n) in file count, O(1) in storage until file modified
  raidz2: ZFS RAID level tolerating 2 simultaneous drive failures; correct choice for consumer SATA drives due to URE probability during resilver
  zfs-native-encryption: AES-256-GCM dataset-level encryption in ZFS; replaces application-level encryption (restic); unlockable via passphrase or keyfile
  two-level-hash-dir: Chunk storage layout ab/12/ab12cd34... — 65,536 subdirectories (256x256), keeps per-directory file count manageable at any scale up to 1TB cap

metadata:
  feature: time-machine
  component: file-protection
  tags: [time-machine, file-protection, fanotify, rsync, nas, zfs, defense-in-depth]

# DEPLOYMENT STATUS (2026-02-10):
# Java daemon v2.0.0 code complete and tested, but deployment failed (jar filename
# mismatch in systemd unit → 5,252 restart loop → service disabled).
# NO FILE PROTECTION CURRENTLY ACTIVE. Needs redeployment.
# Database: 272K versions from Python era, intact.
# Blocking: fix jar path in service unit, re-enable, restart.

stories:
  # === LAYER 1: Reactive versioning ===

  - name: fanotify-captures-every-write
    status: NEEDS_REDEPLOYMENT
    description: Every file close-after-write in $HOME produces a versioned snapshot in the chunk store
    steps:
      - bash: "echo 'tm-e2e-capture-v1' > ~/tm-e2e-capture-test.txt && sleep 2 && echo 'tm-e2e-capture-v2' > ~/tm-e2e-capture-test.txt && sleep 2 && sqlite3 ~/.local/share/time-machine/metadata.db \"SELECT count(*) FROM versions WHERE file_path='/home/bw/tm-e2e-capture-test.txt';\""
    verify:
      - GreaterOrEqual: 2

  - name: fanotify-content-hash-unique
    status: NEEDS_REDEPLOYMENT
    description: Each distinct write produces a version with a unique content hash
    steps:
      - bash: "echo 'hash-test-alpha' > ~/tm-e2e-hash-test.txt && sleep 2 && echo 'hash-test-beta' > ~/tm-e2e-hash-test.txt && sleep 2 && sqlite3 ~/.local/share/time-machine/metadata.db \"SELECT count(DISTINCT content_hash) FROM versions WHERE file_path='/home/bw/tm-e2e-hash-test.txt';\""
    verify:
      - GreaterOrEqual: 2

  - name: fanotify-chunks-on-disk
    status: NEEDS_REDEPLOYMENT
    description: Chunks referenced by a version actually exist on disk
    steps:
      - bash: "echo 'chunk-disk-check' > ~/tm-e2e-chunk-disk.txt && sleep 2 && sqlite3 ~/.local/share/time-machine/metadata.db \"SELECT fc.chunk_hash FROM file_chunks fc JOIN versions v ON fc.version_id = v.id WHERE v.file_path='/home/bw/tm-e2e-chunk-disk.txt' ORDER BY v.id DESC LIMIT 1;\" | while read h; do p=\"${h:0:2}/${h:2:2}/$h\"; test -f ~/.local/share/time-machine/chunks/$p && echo 'exists' || echo 'missing'; done"
    verify:
      - Contains: "exists"

  - name: fanotify-restore-round-trip
    status: NEEDS_REDEPLOYMENT
    description: A file written and captured by the daemon can be reconstructed from chunks to match original content
    steps:
      - bash: "echo 'round-trip-content-12345' > ~/tm-e2e-roundtrip.txt && sleep 2 && VID=$(sqlite3 ~/.local/share/time-machine/metadata.db \"SELECT id FROM versions WHERE file_path='/home/bw/tm-e2e-roundtrip.txt' ORDER BY id DESC LIMIT 1;\") && sudo /usr/lib/jvm/java-25-openjdk/bin/java -Duser.home=/home/bw --enable-native-access=ALL-UNNAMED -jar /home/bw/vault/programs/time-machine/build/libs/time-machine.jar restore /home/bw/tm-e2e-roundtrip.txt --version-id $VID --output /home/bw/tm-e2e-roundtrip-restored.txt 2>&1 && cat /home/bw/tm-e2e-roundtrip-restored.txt"
    verify:
      - Contains: "round-trip-content-12345"

  - name: fastcdc-deduplication
    status: NEEDS_REDEPLOYMENT
    description: Identical content across files produces shared chunks with ref_count > 1
    steps:
      - bash: "python3 -c \"import os; data = os.urandom(1024) * 1024; open(os.path.expanduser('~/tm-e2e-dedup-a.bin'), 'wb').write(data); open(os.path.expanduser('~/tm-e2e-dedup-b.bin'), 'wb').write(data)\" && sleep 3 && sqlite3 ~/.local/share/time-machine/metadata.db \"SELECT max(c.ref_count) FROM chunks c JOIN file_chunks fc ON c.chunk_hash = fc.chunk_hash JOIN versions v ON fc.version_id = v.id WHERE v.file_path = '/home/bw/tm-e2e-dedup-a.bin';\""
    verify:
      - GreaterOrEqual: 2

  - name: exclusion-patterns-enforced
    status: NEEDS_REDEPLOYMENT
    description: Excluded paths never enter the chunk store
    steps:
      - bash: "mkdir -p ~/.cache/tm-e2e-excluded && echo 'should-not-version' > ~/.cache/tm-e2e-excluded/test.txt && sleep 2 && sqlite3 ~/.local/share/time-machine/metadata.db \"SELECT count(*) FROM versions WHERE file_path LIKE '/home/bw/.cache/tm-e2e-excluded%';\""
    verify:
      - Equals: "0"

  - name: exclusion-node-modules
    status: NEEDS_REDEPLOYMENT
    description: node_modules directories are excluded
    steps:
      - bash: "mkdir -p ~/tm-e2e-nm-test/node_modules && echo 'excluded' > ~/tm-e2e-nm-test/node_modules/test.txt && sleep 2 && sqlite3 ~/.local/share/time-machine/metadata.db \"SELECT count(*) FROM versions WHERE file_path LIKE '%tm-e2e-nm-test/node_modules%';\""
    verify:
      - Equals: "0"

  - name: non-excluded-path-captured
    status: NEEDS_REDEPLOYMENT
    description: Files in non-excluded home directories are captured
    steps:
      - bash: "echo 'included-file' > ~/tm-e2e-included.txt && sleep 2 && sqlite3 ~/.local/share/time-machine/metadata.db \"SELECT count(*) FROM versions WHERE file_path='/home/bw/tm-e2e-included.txt';\""
    verify:
      - Greater: 0

  - name: large-file-rejection
    status: NEEDS_REDEPLOYMENT
    description: Files exceeding MAX_FILE_SIZE (100MB) are not stored
    steps:
      - bash: "dd if=/dev/zero of=/home/bw/tm-e2e-largefile.bin bs=1M count=150 2>/dev/null && sleep 3 && sqlite3 ~/.local/share/time-machine/metadata.db \"SELECT count(*) FROM versions WHERE file_path='/home/bw/tm-e2e-largefile.bin';\" && rm -f /home/bw/tm-e2e-largefile.bin"
    verify:
      - Equals: "0"

  - name: process-attribution-captured
    status: NEEDS_REDEPLOYMENT
    description: Version records identify which process modified the file
    steps:
      - bash: "python3 -c \"open('/home/bw/tm-e2e-attribution.txt', 'w').write('from-python')\" && sleep 2 && sqlite3 ~/.local/share/time-machine/metadata.db \"SELECT process_name FROM versions WHERE file_path='/home/bw/tm-e2e-attribution.txt' ORDER BY id DESC LIMIT 1;\""
    verify:
      - Contains: "python"

  - name: process-attribution-pid
    status: NEEDS_REDEPLOYMENT
    description: Process PID is recorded for each version
    steps:
      - bash: "sqlite3 ~/.local/share/time-machine/metadata.db \"SELECT count(*) FROM versions WHERE process_pid IS NOT NULL AND file_path LIKE '/home/bw/%' ORDER BY id DESC LIMIT 100;\""
    verify:
      - Greater: 0

  - name: process-attribution-cmdline
    status: NEEDS_REDEPLOYMENT
    description: Process command line is recorded for each version
    steps:
      - bash: "sqlite3 ~/.local/share/time-machine/metadata.db \"SELECT process_cmdline FROM versions WHERE process_cmdline IS NOT NULL ORDER BY id DESC LIMIT 1;\""
    verify:
      - NotEmpty

  - name: debounce-coalesces-rapid-writes
    status: NEEDS_REDEPLOYMENT
    description: Multiple rapid writes to same file produce fewer versions than writes
    steps:
      - bash: "for i in $(seq 1 20); do echo \"rapid-write-$i\" > /home/bw/tm-e2e-debounce.txt; done && sleep 3 && sqlite3 ~/.local/share/time-machine/metadata.db \"SELECT count(*) FROM versions WHERE file_path='/home/bw/tm-e2e-debounce.txt';\""
    verify:
      # 20 rapid writes should produce significantly fewer than 20 versions due to debounce
      - GreaterOrEqual: 1

  - name: debounce-captures-final-state
    status: NEEDS_REDEPLOYMENT
    description: After debounce fires, the captured version contains the final write content
    steps:
      - bash: "for i in $(seq 1 10); do echo \"debounce-state-$i\" > /home/bw/tm-e2e-debounce-final.txt; done && sleep 3 && VID=$(sqlite3 ~/.local/share/time-machine/metadata.db \"SELECT id FROM versions WHERE file_path='/home/bw/tm-e2e-debounce-final.txt' ORDER BY id DESC LIMIT 1;\") && sudo /usr/lib/jvm/java-25-openjdk/bin/java -Duser.home=/home/bw --enable-native-access=ALL-UNNAMED -jar /home/bw/vault/programs/time-machine/build/libs/time-machine.jar restore /home/bw/tm-e2e-debounce-final.txt --version-id $VID --output /home/bw/tm-e2e-debounce-final-restored.txt 2>&1 && cat /home/bw/tm-e2e-debounce-final-restored.txt"
    verify:
      - Contains: "debounce-state-10"

  - name: rename-preserves-history
    status: NOT_IMPLEMENTED
    description: File rename creates path alias; queries follow rename chain
    steps:
      # TODO(rename-tracking): fanotify MOVED_FROM/MOVED_TO events not yet wired to path_aliases
      - bash: "echo 'TODO: rename tracking not yet implemented'"
      # - bash: "echo 'original-rename' > ~/tm-e2e-rename-a.txt && sleep 2 && mv ~/tm-e2e-rename-a.txt ~/tm-e2e-rename-b.txt && sleep 2 && sudo /usr/lib/jvm/java-25-openjdk/bin/java -Duser.home=/home/bw --enable-native-access=ALL-UNNAMED -jar /home/bw/vault/programs/time-machine/build/libs/time-machine.jar list /home/bw/tm-e2e-rename-b.txt 2>&1"
    verify:
      # TODO: should show versions from when file was named tm-e2e-rename-a.txt via path_aliases
      - Contains: "TODO"

  - name: delete-marks-timeline
    status: NOT_IMPLEMENTED
    description: File deletion records a delete marker without losing previous versions
    steps:
      # TODO(delete-markers): FAN_DELETE events not yet generating operation='delete' versions
      - bash: "echo 'TODO: delete markers not yet implemented'"
      # - bash: "echo 'before-delete' > ~/tm-e2e-delete.txt && sleep 2 && rm ~/tm-e2e-delete.txt && sleep 2 && sqlite3 ~/.local/share/time-machine/metadata.db \"SELECT operation FROM versions WHERE file_path='/home/bw/tm-e2e-delete.txt' ORDER BY id DESC LIMIT 1;\""
    verify:
      # TODO: last version should have operation='delete'
      - Contains: "TODO"

  - name: atomic-write-crash-recovery
    status: DATA_INTACT
    description: Kill daemon mid-write; restart verifies no corruption
    steps:
      - bash: "sqlite3 ~/.local/share/time-machine/metadata.db \"SELECT count(*) FROM versions v LEFT JOIN file_chunks fc ON v.id = fc.version_id LEFT JOIN chunks c ON fc.chunk_hash = c.chunk_hash WHERE fc.chunk_hash IS NULL AND v.chunk_count > 0;\""
    verify:
      # Zero broken references (metadata pointing to missing chunks)
      - Equals: "0"

  - name: sqlite-wal-integrity
    status: DATA_INTACT
    description: SQLite database passes integrity check
    steps:
      - bash: "sqlite3 ~/.local/share/time-machine/metadata.db 'PRAGMA integrity_check;'"
    verify:
      - Equals: "ok"

  - name: daemon-service-running
    status: NEEDS_REDEPLOYMENT
    description: time-machine.service is active and running
    steps:
      - bash: "systemctl is-active time-machine.service"
    verify:
      - Equals: "active"

  - name: daemon-memory-bounded
    status: NEEDS_REDEPLOYMENT
    description: Daemon memory stays within configured limit
    steps:
      - bash: "systemctl show time-machine.service --property=MemoryPeak --value | awk '{printf \"%.0f\", $1/1048576}'"
    verify:
      # Memory peak in MB should be under 1024 (1GB limit)
      - GreaterOrEqual: 0

  - name: fanotify-burst-throughput
    status: NEEDS_REDEPLOYMENT
    description: Daemon keeps up with rapid file creation without event loss
    steps:
      - bash: "mkdir -p ~/tm-e2e-burst && for i in $(seq 1 100); do echo \"burst-$i\" > ~/tm-e2e-burst/file-$i.txt; done && sleep 10 && sqlite3 ~/.local/share/time-machine/metadata.db \"SELECT count(*) FROM versions WHERE file_path LIKE '/home/bw/tm-e2e-burst/file-%';\""
    verify:
      # All 100 files should have at least one version (debounce may coalesce some)
      - GreaterOrEqual: 90

  - name: migration-from-python-daemon
    status: DATA_INTACT
    description: Import from Python file-version-daemon-v3 was completed successfully
    steps:
      - bash: "sqlite3 ~/.local/share/time-machine/metadata.db \"SELECT count(*) FROM versions;\""
    verify:
      # 255K+ versions imported from Python daemon plus new Java versions
      - Greater: 255000

  - name: cross-implementation-chunk-identity
    status: DATA_INTACT
    description: Python cdc.py and Java FastCDC produce identical chunks for same input
    steps:
      # Migration is complete and dedup works across boundary, verifiable by checking
      # that chunks from early (imported) versions are shared with recent versions
      - bash: "sqlite3 ~/.local/share/time-machine/metadata.db \"SELECT count(*) FROM chunks WHERE ref_count > 1;\""
    verify:
      - Greater: 0

  # === LAYER 2: Periodic hardlink snapshots ===

  - name: snapshot-timer-active
    status: NEEDS_REDEPLOYMENT
    description: Snapshot timer is enabled and running
    steps:
      - bash: "systemctl is-active time-machine-snapshot.timer"
    verify:
      - Contains: "active"

  - name: hardlink-snapshot-creates-tree
    status: NEEDS_REDEPLOYMENT
    description: Timer creates timestamped hardlink snapshot directories
    steps:
      - bash: "ls -d ~/.local/share/time-machine/snapshots/????-??-??_??-?? 2>/dev/null | wc -l"
    verify:
      - Greater: 0

  - name: hardlink-snapshot-contains-files
    status: NEEDS_REDEPLOYMENT
    description: Snapshot directories contain files from $HOME as hardlinks
    steps:
      - bash: "LATEST=$(ls -d ~/.local/share/time-machine/snapshots/????-??-??_??-?? 2>/dev/null | tail -1) && test -f \"$LATEST/.bashrc\" && stat -c '%h' \"$LATEST/.bashrc\""
    verify:
      # Hardlink count should be > 1 (linked to original)
      - Greater: 1

  - name: hardlink-snapshot-excludes-cache
    status: NEEDS_REDEPLOYMENT
    description: Excluded directories (.cache) are absent from snapshots
    steps:
      - bash: "LATEST=$(ls -d ~/.local/share/time-machine/snapshots/????-??-??_??-?? 2>/dev/null | tail -1) && test -d \"$LATEST/.cache\" && echo 'present' || echo 'absent'"
    verify:
      - Equals: "absent"

  - name: hardlink-snapshot-independent-of-daemon
    status: NEEDS_REDEPLOYMENT
    description: Snapshot timer does not depend on time-machine.service
    steps:
      - bash: "systemctl show time-machine-snapshot.service --property=After --value | grep -c 'time-machine.service' || echo '0'"
    verify:
      - Equals: "0"

  - name: hardlink-snapshot-24h-retention
    status: NEEDS_REDEPLOYMENT
    description: No snapshots older than 24 hours exist
    steps:
      - bash: "find ~/.local/share/time-machine/snapshots -maxdepth 1 -type d -name '????-??-??_??-??' -mmin +1445 2>/dev/null | wc -l"
    verify:
      - Equals: "0"

  - name: snapshot-script-exists
    status: NEEDS_REDEPLOYMENT
    description: The bash snapshot script exists and is executable
    steps:
      - bash: "test -x ~/vault/programs/time-machine/bin/snapshot.sh && echo 'executable'"
    verify:
      - Equals: "executable"

  - name: snapshot-uses-rsync-link-dest
    status: NEEDS_REDEPLOYMENT
    description: Snapshot script uses rsync --link-dest for hardlink dedup
    steps:
      - bash: "grep -c 'link-dest\\|link_dest' ~/vault/programs/time-machine/bin/snapshot.sh"
    verify:
      - Greater: 0

  # === LAYER 3: Off-site archive (NAS) ===

  - name: archive-timer-active
    status: NOT_IMPLEMENTED
    description: Archive timer is enabled and scheduled
    steps:
      - bash: "systemctl is-active time-machine-archive.timer"
    verify:
      - Contains: "active"

  - name: archive-script-exists
    status: NOT_IMPLEMENTED
    description: The bash archive script exists and is executable
    steps:
      - bash: "test -x ~/vault/programs/time-machine/bin/archive.sh && echo 'executable'"
    verify:
      - Equals: "executable"

  - name: archive-script-uses-rsync
    status: NOT_IMPLEMENTED
    description: Archive script uses rsync over SSH for chunk transfer to NAS
    steps:
      - bash: "grep -c 'rsync' ~/vault/programs/time-machine/bin/archive.sh"
    verify:
      - Greater: 0

  - name: archive-script-excludes-cache
    status: NOT_IMPLEMENTED
    description: Archive script excludes .cache from backup
    steps:
      - bash: "grep -c '\\.cache' ~/vault/programs/time-machine/bin/archive.sh"
    verify:
      - Greater: 0

  - name: archive-config-target
    status: NOT_IMPLEMENTED
    description: Archive target is configured in config.toml
    steps:
      - bash: "grep 'target' ~/.config/time-machine/config.toml"
    verify:
      - Contains: "nas:"

  - name: archive-config-retention
    status: NOT_IMPLEMENTED
    description: 600-day retention policy is configured
    steps:
      - bash: "grep 'layer3_days' ~/.config/time-machine/config.toml"
    verify:
      - Contains: "600"

  - name: rsync-daily-backup-to-nas
    status: NOT_IMPLEMENTED
    description: Daily timer pushes chunk store and metadata to NAS via rsync over SSH
    steps:
      # TODO(layer3-nas): NAS not yet connected
      - bash: "echo 'TODO: NAS not yet connected'"
    verify:
      - Contains: "TODO"

  - name: rsync-survives-local-destruction
    status: NOT_IMPLEMENTED
    description: After sudo rm -rf /home, full recovery possible from NAS
    steps:
      # TODO(layer3-nas): NAS not yet connected; cannot test restore from NAS
      - bash: "echo 'TODO: NAS not yet connected'"
    verify:
      - Contains: "TODO"

  - name: zfs-encryption-at-rest
    status: NOT_IMPLEMENTED
    description: Data on NAS is encrypted via ZFS native encryption (AES-256-GCM)
    steps:
      # TODO(layer3-nas): NAS not yet connected; cannot verify encryption
      - bash: "echo 'TODO: NAS not yet connected'"
      # - bash: "ssh nas 'zfs get encryption backup/time-machine' | grep -c 'aes-256-gcm'"
    verify:
      - Contains: "TODO"

  - name: zfs-scrub-integrity
    status: NOT_IMPLEMENTED
    description: Weekly ZFS scrub verifies integrity of all data on NAS
    steps:
      # TODO(layer3-nas): NAS not yet connected; cannot verify scrub
      - bash: "echo 'TODO: NAS not yet connected'"
      # - bash: "ssh nas 'zpool status backup' | grep -c 'scrub repaired'"
    verify:
      - Contains: "TODO"

  - name: rsync-on-demand-push
    status: NOT_IMPLEMENTED
    description: User can force immediate backup outside daily schedule via push CLI
    steps:
      # TODO(layer3-nas): NAS not yet connected; push command configured but untestable
      - bash: "echo 'TODO: NAS not yet connected'"
    verify:
      - Contains: "TODO"

  # === RECOVERY ===

  - name: cli-list-shows-versions
    status: IMPLEMENTED
    description: time-machine list command shows Layer 1 versions for a tracked file
    steps:
      - bash: "sudo /usr/lib/jvm/java-25-openjdk/bin/java -Duser.home=/home/bw --enable-native-access=ALL-UNNAMED -jar /home/bw/vault/programs/time-machine/build/libs/time-machine.jar list /home/bw/.bashrc 2>&1"
    verify:
      - Contains: "modify"

  - name: cli-restore-by-version-id
    status: IMPLEMENTED
    description: time-machine restore recovers a specific version by ID
    steps:
      - bash: "echo 'restore-test-content' > ~/tm-e2e-cli-restore.txt && sleep 2 && VID=$(sqlite3 ~/.local/share/time-machine/metadata.db \"SELECT id FROM versions WHERE file_path='/home/bw/tm-e2e-cli-restore.txt' ORDER BY id DESC LIMIT 1;\") && sudo /usr/lib/jvm/java-25-openjdk/bin/java -Duser.home=/home/bw --enable-native-access=ALL-UNNAMED -jar /home/bw/vault/programs/time-machine/build/libs/time-machine.jar restore /home/bw/tm-e2e-cli-restore.txt --version-id $VID --output /home/bw/tm-e2e-cli-restored.txt 2>&1 && cat /home/bw/tm-e2e-cli-restored.txt"
    verify:
      - Contains: "restore-test-content"

  - name: cli-restore-output-path
    status: IMPLEMENTED
    description: Restore --output writes to specified path instead of overwriting original
    steps:
      - bash: "echo 'output-path-test' > ~/tm-e2e-output-path.txt && sleep 2 && VID=$(sqlite3 ~/.local/share/time-machine/metadata.db \"SELECT id FROM versions WHERE file_path='/home/bw/tm-e2e-output-path.txt' ORDER BY id DESC LIMIT 1;\") && sudo /usr/lib/jvm/java-25-openjdk/bin/java -Duser.home=/home/bw --enable-native-access=ALL-UNNAMED -jar /home/bw/vault/programs/time-machine/build/libs/time-machine.jar restore /home/bw/tm-e2e-output-path.txt --version-id $VID --output /home/bw/tm-e2e-output-alt.txt 2>&1 && test -f /home/bw/tm-e2e-output-alt.txt && echo 'output-file-exists'"
    verify:
      - Contains: "output-file-exists"

  - name: cli-stats-shows-layers
    status: IMPLEMENTED
    description: time-machine stats shows storage info for all layers
    steps:
      - bash: "sudo /usr/lib/jvm/java-25-openjdk/bin/java -Duser.home=/home/bw --enable-native-access=ALL-UNNAMED -jar /home/bw/vault/programs/time-machine/build/libs/time-machine.jar stats 2>&1"
    verify:
      - Contains: "Layer 1"
      - Contains: "Layer 2"
      - Contains: "Dedup ratio"

  - name: cli-stats-version-count
    status: IMPLEMENTED
    description: Stats shows correct version count
    steps:
      - bash: "sudo /usr/lib/jvm/java-25-openjdk/bin/java -Duser.home=/home/bw --enable-native-access=ALL-UNNAMED -jar /home/bw/vault/programs/time-machine/build/libs/time-machine.jar stats 2>&1 | grep 'Versions stored'"
    verify:
      - NotEmpty

  - name: unified-recovery-search
    status: NOT_IMPLEMENTED
    description: Single command searches all three layers for file versions
    steps:
      # TODO(unified-search): list command currently only queries Layer 1 (SQLite)
      - bash: "echo 'TODO: unified search across all 3 layers not yet implemented'"
      # - bash: "sudo /usr/lib/jvm/java-25-openjdk/bin/java -Duser.home=/home/bw --enable-native-access=ALL-UNNAMED -jar /home/bw/vault/programs/time-machine/build/libs/time-machine.jar list /home/bw/.bashrc 2>&1 | grep -c 'L[123]'"
    verify:
      # TODO: should show versions from L1, L2, and L3 labeled by source
      - Contains: "TODO"

  - name: recovery-from-layer-2
    status: IMPLEMENTED
    description: Restore file from hardlink snapshot when Layer 1 unavailable
    steps:
      - bash: "LATEST=$(ls -d ~/.local/share/time-machine/snapshots/????-??-??_??-?? 2>/dev/null | tail -1) && test -f \"$LATEST/.bashrc\" && cat \"$LATEST/.bashrc\" | head -1"
    verify:
      - NotEmpty

  - name: recovery-from-layer-3
    status: NOT_IMPLEMENTED
    description: Restore file from NAS when all local data destroyed
    steps:
      # TODO(layer3-nas): NAS not yet connected; cannot test NAS restore
      - bash: "echo 'TODO: NAS not yet connected'"
    verify:
      - Contains: "TODO"

  - name: recovery-directory-tree
    status: NOT_IMPLEMENTED
    description: Restore entire directory recursively from any layer
    steps:
      # TODO(recursive-restore): recursive restore not yet implemented in CLI
      - bash: "echo 'TODO: recursive restore not yet implemented'"
    verify:
      - Contains: "TODO"

  # === OPERATIONAL ===

  - name: config-file-valid
    status: IMPLEMENTED
    description: config.toml exists and contains all required sections
    steps:
      - bash: "grep -c '\\[daemon\\]\\|\\[exclusions\\]\\|\\[retention\\]\\|\\[snapshot\\]\\|\\[archive\\]\\|\\[resources\\]' ~/.config/time-machine/config.toml"
    verify:
      - GreaterOrEqual: 6

  - name: config-debounce-values
    status: IMPLEMENTED
    description: Debounce is configured with 100ms quiet period and 1s max delay
    steps:
      - bash: "grep 'quiet_ms' ~/.config/time-machine/config.toml | grep -o '[0-9]*'"
    verify:
      - Equals: "100"

  - name: config-debounce-max-delay
    status: IMPLEMENTED
    description: Max delay is 1000ms
    steps:
      - bash: "grep 'max_delay_ms' ~/.config/time-machine/config.toml | grep -o '[0-9]*'"
    verify:
      - Equals: "1000"

  - name: config-chunking-fastcdc
    status: IMPLEMENTED
    description: Chunking algorithm is FastCDC
    steps:
      - bash: "grep 'algorithm' ~/.config/time-machine/config.toml"
    verify:
      - Contains: "fastcdc"

  - name: config-chunking-sizes
    status: IMPLEMENTED
    description: Chunk sizes are 8/16/32KB
    steps:
      - bash: "grep 'min_size\\|avg_size\\|max_size' ~/.config/time-machine/config.toml | tr '\\n' ' '"
    verify:
      - Contains: "8192"
      - Contains: "16384"
      - Contains: "32768"

  - name: config-compression-zstd
    status: IMPLEMENTED
    description: Compression is zstd level 3
    steps:
      - bash: "grep 'compression' ~/.config/time-machine/config.toml | head -1"
    verify:
      - Contains: "zstd"

  - name: retention-layer1-90days
    status: IMPLEMENTED
    description: Layer 1 retention is configured for 90 days
    steps:
      - bash: "grep 'layer1_days' ~/.config/time-machine/config.toml | grep -o '[0-9]*'"
    verify:
      - Equals: "90"

  - name: retention-layer2-24hours
    status: IMPLEMENTED
    description: Layer 2 retention is 24 hours (1440 minutes)
    steps:
      - bash: "grep 'layer2_minutes' ~/.config/time-machine/config.toml | grep -o '[0-9]*'"
    verify:
      - Equals: "1440"

  - name: retention-layer3-600days
    status: IMPLEMENTED
    description: Layer 3 retention is 600 days
    steps:
      - bash: "grep 'layer3_days' ~/.config/time-machine/config.toml | grep -o '[0-9]*'"
    verify:
      - Equals: "600"

  - name: resource-limits-memory
    status: IMPLEMENTED
    description: Daemon memory limit is set to 1G
    steps:
      - bash: "grep 'daemon_memory_max' ~/.config/time-machine/config.toml"
    verify:
      - Contains: "1G"

  - name: resource-limits-cpu
    status: IMPLEMENTED
    description: Resource CPU quotas are configured per layer
    steps:
      - bash: "grep 'cpu_quota' ~/.config/time-machine/config.toml | tr '\\n' ' '"
    verify:
      - Contains: "30%"
      - Contains: "10%"
      - Contains: "50%"

  - name: storage-dedup-effective
    status: IMPLEMENTED
    description: Dedup ratio demonstrates significant storage savings
    steps:
      - bash: "sudo /usr/lib/jvm/java-25-openjdk/bin/java -Duser.home=/home/bw --enable-native-access=ALL-UNNAMED -jar /home/bw/vault/programs/time-machine/build/libs/time-machine.jar stats 2>&1 | grep 'Dedup ratio' | grep -oP '[0-9]+\\.?[0-9]*'"
    verify:
      # Dedup ratio should be significant (current is 105.7x)
      - Greater: 1

  - name: database-indices-exist
    status: IMPLEMENTED
    description: Required indices exist for query performance
    steps:
      - bash: "sqlite3 ~/.local/share/time-machine/metadata.db \".indices\" | wc -l"
    verify:
      - GreaterOrEqual: 5

  - name: fanotify-mount-point
    status: IMPLEMENTED
    description: Fanotify monitors root filesystem for full coverage
    steps:
      - bash: "grep 'mount_point' ~/.config/time-machine/config.toml"
    verify:
      - Contains: "\"/\""

  - name: staging-dir-configured
    status: IMPLEMENTED
    description: Staging directory is configured for atomic writes
    steps:
      - bash: "grep 'staging_dir' ~/.config/time-machine/config.toml"
    verify:
      - Contains: ".staging"

  - name: schema-version-tracked
    status: IMPLEMENTED
    description: Database schema version is tracked
    steps:
      - bash: "sqlite3 ~/.local/share/time-machine/metadata.db \"SELECT value FROM schema_info WHERE key='version';\""
    verify:
      - NotEmpty

  - name: retention-cleanup
    status: DATA_INTACT
    description: Each layer enforces its retention policy automatically
    steps:
      # Layer 1 has 90-day retention; verify no ancient versions exist
      - bash: "sqlite3 ~/.local/share/time-machine/metadata.db \"SELECT count(*) FROM versions WHERE timestamp < strftime('%s','now') - (91 * 86400);\""
    verify:
      - Equals: "0"

  - name: storage-monitoring
    status: IMPLEMENTED
    description: Stats command reports storage usage for each layer
    steps:
      - bash: "sudo /usr/lib/jvm/java-25-openjdk/bin/java -Duser.home=/home/bw --enable-native-access=ALL-UNNAMED -jar /home/bw/vault/programs/time-machine/build/libs/time-machine.jar stats 2>&1 | grep -c 'GB\\|MB'"
    verify:
      - Greater: 0

enforced_constraints:
  - name: three-independent-layers
    value: Layer 1 (fanotify/chunks), Layer 2 (hardlink snapshots), Layer 3 (rsync to NAS)
    rationale: Each layer has independent failure modes; no single failure (daemon crash, database corruption, local disk destruction) causes irrecoverable data loss

  - name: layer-3-off-machine
    value: NAS only — no local-only archive
    rationale: Under threat model (LLM with sudo), any local path is destroyable; Layer 3 must be physically unreachable from workstation filesystem

  - name: zfs-native-encryption
    value: mandatory AES-256-GCM on NAS dataset via ZFS native encryption
    rationale: NAS is another attack surface; data at rest must be encrypted; ZFS native encryption provides dataset-level AES-256-GCM with per-dataset keys; replaces restic application-level encryption (ADR-023)

  - name: zfs-scrub-weekly
    value: weekly zfs scrub on NAS pool
    rationale: Detects bit rot on NAS storage before it causes unrecoverable data loss; NAS drives are consumer-grade; ZFS scrub checks every block including copies and parity; more thorough than application-level verification

  - name: two-level-hash-chunk-dir
    value: Chunks stored at chunkDir/ab/12/ab12cd34... (2-level hash prefix directory)
    rationale: Flat directory with 200M files (at 1TB cap) exceeds ext4 htree limits; 2-level hash creates 65,536 subdirectories (~3,050 files each at max scale); rsync scan time stays manageable

  - name: storage-cap-1tb
    value: Layer 1 chunk store capped at 1TB with LRU eviction
    rationale: Unbounded growth on 2TB NVMe is unacceptable. At 90-day retention with typical developer workload (5-10GB raw/day, ~500MB compressed/day after dedup), 90 days is ~45GB. Pathological workload (100GB raw/day, 10GB compressed) reaches ~900GB in 90 days. 1TB cap with LRU eviction at 900GB (90%) guarantees disk space for OS and other software. Warning at 800GB (80%). Eviction removes oldest versions first regardless of retention setting.

  - name: nas-zfs-raidz2
    value: 4 drives, RAIDZ2 (2-drive parity)
    rationale: Consumer SATA drives have URE rate ~1 in 10^14 bits; during resilver of 2TB drive across 6TB pool, probability of hitting URE is non-negligible; RAIDZ2 tolerates this; RAIDZ1 does not

  - name: sub-second-layer-1-latency
    value: file write to version available in < 1 second
    rationale: Recovery usefulness degrades if versions lag behind writes; sub-second ensures the version captured before accidental overwrite

  - name: layer-2-daemon-independent
    value: hardlink snapshots must not depend on Layer 1 daemon, database, or chunk store
    rationale: Layer 2 exists precisely to cover Layer 1 software failures; any dependency on Layer 1 defeats the purpose

  - name: fanotify-not-inotify
    value: kernel fanotify, not inotify
    rationale: inotify has ~8K watch limit per process, does not scale to full $HOME monitoring; fanotify has no watch limit, kernel-guaranteed delivery

  - name: java-not-python
    value: time-machine daemon is Java (JDK 25), not Python
    rationale: FastCDC inner loop is 10-50x faster under JIT than CPython interpreter; virtual threads for concurrent NAS replication; Panama FFI for fanotify syscall; type safety reduces crash frequency (current Python daemon: 431 restarts)

opinionated_constraints:
  - name: rsync-over-application-backup
    description: Use rsync for NAS push, not application-level backup tools (restic, borg, etc.)
    rationale: Chunk store is already content-addressed — rsync transfers only new/changed chunks. ZFS native encryption provides at-rest encryption. ZFS scrub provides integrity verification. Application-level backup tools duplicate content-addressing (their CDC produces zero marginal dedup on 8-32KB chunks) and add a Go/Python runtime to a C++/Java stack. rsync is a system package, 20+ year track record, zero runtime dependencies. See ADR-023.

  - name: no-abstract-storage-backend
    description: Drop the StorageBackend abstraction from the existing PRD
    rationale: The PRD designed a polymorphic storage backend (LocalStorage, CephStorage) for future Ceph migration. The NAS layer uses rsync over SSH, not Ceph RADOS. The abstraction adds indirection for a migration that is no longer planned. Layer 1 writes chunks to local filesystem. Layer 3 runs rsync. No polymorphism needed.

  - name: hardlink-snapshots-retained
    description: Layer 2 hardlink snapshots are kept despite Layer 1 covering every write
    rationale: Layer 2 protects against Layer 1 software defects (CDC bugs, database corruption, daemon crashes). Independent software path to the same data. 36GB on 2TB NVMe is 1.8% of capacity. Eliminating an independent failure path to save 1.8% disk is unacceptable.

  - name: nas-zfs-bare-cli
    description: NAS runs Arch or Debian with ZFS installed from packages, managed via CLI
    rationale: TrueNAS hides ZFS operations behind a web UI. The user's goal is learning scalable multi-drive storage. CLI-only ZFS (zpool, zfs, zdb) provides the deepest learning surface. No firmware-magic appliances.

  - name: chunk-store-included-in-layer-3
    description: rsync archive includes ~/.local/share/time-machine/ (chunk store + metadata DB)
    rationale: Restoring from NAS recovers not just the chunk store but also the SQLite metadata database, enabling per-write granularity recovery going back 90 days from last backup

  - name: gear-table-identity
    description: Java FastCDC uses identical gear hash table as Python cdc.py
    rationale: Different gear table = different chunk boundaries = broken dedup across Python→Java migration. 255K existing versions become separate dedup domain. Storage doubles at migration boundary.

  - name: debounce-not-rate-limit
    description: Event coalescing via 100ms debounce with 1s max delay, not per-file rate limiting
    rationale: Rate limiting drops versions, violating "every file at every version." Debounce captures final state after write burst settles. 100ms collapses atomic-save patterns (write tmp + rename). 1s max prevents infinite deferral under continuous write.

  - name: atomic-write-protocol
    description: Chunks written and finalized to disk before SQLite metadata commits
    rationale: No state can exist where metadata references missing chunks. Orphan chunks (chunks without metadata) are harmless and cleaned on startup. Missing chunks (metadata without files) cause corrupt recovery. The protocol makes orphans possible, missing chunks impossible.

  - name: single-writer-thread
    description: All SQLite writes go through one dedicated thread via MPSC queue
    rationale: SQLite WAL allows one writer + unlimited readers. Single writer eliminates contention. Virtual threads submit completed version data to queue. Readers (CLI, daemon queries) are lock-free.

  - name: process-attribution
    description: Every version records PID, process name, and cmdline of the writing process
    rationale: Enables "which LLM agent overwrote my config?" forensics. Read from /proc/<pid>/ in event handler immediately (before process exits). Null if process already exited.

  - name: path-alias-tracking
    description: File renames tracked in path_aliases table; queries follow rename chains
    rationale: Without alias tracking, renaming A→B loses all history when querying B. Alias chain allows time-machine list B to show versions from when the file was named A.

  - name: layer-2-bash-not-java
    description: Layer 2 snapshot script is bash + rsync, not part of the Java daemon
    rationale: Maximum independence from Layer 1. If the JVM crashes, bash+rsync still runs via systemd timer. JVM startup cost (~200ms) is wasteful for a 1-second operation. Proven tools (rsync --link-dest, 20+ year track record).
