vocabulary:
  agm_belief_revision: "AGM (Alchourron-Gardenfors-Makinson) framework for rational belief change. Beliefs are revised by contracting (removing) conflicting beliefs before expanding (adding) new ones."
  contradiction_resolver: "Functional interface that takes two conflicting beliefs and returns a ContradictionResult with resolution action and explanation event."
  ontology_prefix: "Dot-separated category namespace (constraints.*, preferences.*, models.*, heuristics.*) that determines which resolver handles a contradiction."
  constraint_resolver: "Resolver for constraints.* beliefs. High scrutiny — always flags for manual review regardless of content."
  preference_resolver: "Resolver for preferences.* beliefs. User-mediated — surfaces both beliefs to user for decision."
  model_resolver: "Resolver for models.* beliefs. Last-write-wins — newer belief replaces older without review."
  heuristic_resolver: "Resolver for heuristics.* beliefs. Merges compatible beliefs; detects conflicts via negation markers and shared-term analysis."
  negation_marker: "Words indicating opposition (not, never, avoid, without, etc.) used by heuristicResolver to detect semantic conflicts between beliefs."
  shared_term: "A word >4 characters that appears in both beliefs. heuristicResolver requires >=2 shared terms for conflict detection."
  bootstrap_events: "One-time idempotent process that reads CLAUDE.md and MEMORY.md to create foundational SYSTEM events and backfills from session key_facts."
  sha256_event_id: "Deterministic event ID generated by SHA-256 hashing of event content, ensuring identical content always produces the same ID for deduplication."
  backfill_guard: "Tag-based check preventing duplicate backfill of session key_facts. If a session already has the backfill tag, it is skipped."
  event_emitter: "Fire-and-forget service that writes SYSTEM events to PostgreSQL (required) and optionally Kafka, using virtual threads."
  dual_write: "Pattern where data is written to PostgreSQL as primary store and Kafka as optional secondary. PostgreSQL failure is fatal; Kafka failure is logged and ignored."
  turn_capture: "Service that records conversation turns (user/agent/system) with UUID, timestamp, truncated content, and optional tool metadata."
  max_content_length: "500-character hard limit for turn content in TurnCapture. Content exceeding this is truncated."
  virtual_thread: "JDK 25 lightweight thread (Project Loom). Used by EventEmitter for non-blocking fire-and-forget writes."
  system_origin: "Event origin value 'SYSTEM' applied to all programmatically generated events (bootstrap, emitter). Distinguishes from user-authored events."
  on_conflict_do_nothing: "SQL INSERT behavior that silently skips rows with duplicate primary keys, enabling idempotent writes."

metadata:
  feature: Data Pipeline Emission and Extraction
  component: datapipeline
  date: 2026-02-19

stories:
  # ── ContradictionResolver: Dispatch and Prefix Routing ──

  - name: contradiction-dispatch-constraints-prefix
    description: >
      A contradiction in the constraints.* namespace routes to constraintResolver,
      which always flags for manual review regardless of belief content.
    preconditions:
      - "Two beliefs exist with category 'constraints.system-limits'"
      - "Beliefs have conflicting content (e.g., 'max connections = 100' vs 'max connections = 50')"
    steps:
      - "ContradictionResolver.resolve() is called with both beliefs and category 'constraints.system-limits'"
      - "Prefix extraction yields 'constraints' from the dotted category"
      - "Dispatch selects constraintResolver"
      - "constraintResolver returns MANUAL_REVIEW action"
    verify:
      - "Result action is MANUAL_REVIEW, not AUTO_RESOLVE or MERGE"
      - "Result contains an explanation event with SYSTEM origin"
      - "Explanation event statement is >= 50 characters"
      - "Both original beliefs are preserved (neither discarded)"

  - name: contradiction-dispatch-models-last-write-wins
    description: >
      A contradiction in models.* namespace uses modelResolver which applies
      last-write-wins — the newer belief replaces the older one without review.
    preconditions:
      - "Belief A (older timestamp) in category 'models.user-behavior'"
      - "Belief B (newer timestamp) in same category with conflicting content"
    steps:
      - "ContradictionResolver.resolve() called with beliefs A, B and category 'models.user-behavior'"
      - "Dispatch selects modelResolver based on 'models' prefix"
      - "modelResolver compares timestamps and selects belief B"
    verify:
      - "Result action is AUTO_RESOLVE"
      - "Winning belief is B (the newer one)"
      - "Explanation event has SYSTEM origin and statement >= 50 chars"
      - "Losing belief A is marked for contraction"

  - name: contradiction-dispatch-null-category-defaults-constraint
    description: >
      When category is null or blank, dispatch defaults to constraintResolver
      (highest scrutiny) as a safety fallback.
    preconditions:
      - "Two conflicting beliefs exist"
      - "Category is null (or empty string, or whitespace-only)"
    steps:
      - "ContradictionResolver.resolve() called with category = null"
      - "Prefix extraction handles null safely, returns empty/default"
      - "Dispatch falls through to constraintResolver as default"
    verify:
      - "No NullPointerException thrown"
      - "Result action is MANUAL_REVIEW (constraint behavior)"
      - "Explanation event is generated with valid SYSTEM origin"
      - "Same behavior for empty string '' and whitespace ' ' categories"

  - name: contradiction-heuristic-compatible-merge
    description: >
      heuristicResolver merges two compatible beliefs — same domain,
      no negation markers, complementary content. The merge result must
      preserve both original belief texts verbatim.
    preconditions:
      - "Belief A: 'PostgreSQL connection pooling improves throughput under concurrent load'"
      - "Belief B: 'PostgreSQL connection pooling benefits from tuning pool size to match CPU cores'"
      - "Category: 'heuristics.database-tuning'"
    steps:
      - "Dispatch selects heuristicResolver for 'heuristics' prefix"
      - "Negation marker scan finds no markers in either belief"
      - "Shared term analysis finds 'PostgreSQL', 'connection', 'pooling' (3 terms > 4 chars)"
      - ">=2 shared terms + no negation = compatible, triggers MERGE"
      - "Inspect the merged result content"
    verify:
      - "Result action is MERGE"
      - "Both beliefs are retained and combined"
      - "Merged result contains belief A text verbatim — no paraphrasing or summarization"
      - "Merged result contains belief B text verbatim — neither content lost"
      - "Explanation event describes the merge rationale"
      - "No manual review flagged"

  - name: contradiction-heuristic-conflict-negation-detected
    description: >
      heuristicResolver detects conflict when beliefs share terms but one
      contains negation markers, indicating semantic opposition.
    preconditions:
      - "Belief A: 'Virtual threads should always be used for database operations'"
      - "Belief B: 'Virtual threads should never be used for database operations with synchronized blocks'"
      - "Category: 'heuristics.concurrency'"
    steps:
      - "Dispatch selects heuristicResolver"
      - "Negation scan detects 'never' in belief B"
      - "Shared term analysis finds 'Virtual', 'threads', 'database', 'operations' (4 terms > 4 chars)"
      - ">=2 shared terms + negation marker = conflict detected"
    verify:
      - "Result action is MANUAL_REVIEW (conflict escalation)"
      - "Explanation event references the negation marker found"
      - "Both beliefs preserved pending user decision"

  - name: contradiction-heuristic-insufficient-shared-terms
    description: >
      heuristicResolver requires >=2 shared terms of >4 chars. When beliefs
      share fewer than 2 qualifying terms, they are treated as unrelated
      and merged without conflict detection.
    preconditions:
      - "Belief A: 'Emacs keybindings reduce context switching'"
      - "Belief B: 'Never use recursive grep on large repositories'"
      - "Category: 'heuristics.workflow'"
    steps:
      - "Dispatch selects heuristicResolver"
      - "Shared term analysis: no words >4 chars appear in both beliefs"
      - "Shared term count < 2, so conflict detection is skipped"
      - "Despite 'Never' negation marker in B, insufficient overlap means no conflict"
    verify:
      - "Result action is MERGE (not conflict)"
      - "Beliefs treated as addressing different domains"
      - "Negation marker alone is insufficient without shared-term threshold"

  - name: contradiction-heuristic-word-length-boundary
    description: >
      heuristicResolver counts shared terms with >4 characters (strictly greater
      than, not >=4). Words of exactly 4 chars must NOT count as significant.
      This tests the off-by-one boundary between 4 and 5 character words.
    preconditions:
      - "Belief A: 'The data pool uses fast sync mode for safe writes'"
      - "Belief B: 'The data pool uses fast sync mode but never for bulk imports'"
      - "Category: 'heuristics.storage'"
      - "Shared words: 'data' (4 chars), 'pool' (4 chars), 'uses' (4 chars), 'fast' (4 chars), 'sync' (4 chars), 'mode' (4 chars)"
    steps:
      - "Dispatch selects heuristicResolver for 'heuristics' prefix"
      - "Shared term analysis identifies words appearing in both beliefs"
      - "Filter by length: 'data' (4) excluded, 'pool' (4) excluded, 'uses' (4) excluded, 'fast' (4) excluded, 'sync' (4) excluded, 'mode' (4) excluded"
      - "All shared words are exactly 4 chars — none exceed the >4 threshold"
      - "Qualifying shared term count is 0 (< 2 required)"
    verify:
      - "Result action is MERGE (insufficient shared terms for conflict detection)"
      - "Words of exactly 4 characters are NOT counted as significant (>4, not >=4)"
      - "Despite 'never' negation marker in belief B, insufficient overlap means no conflict"
      - "Changing 'sync' to 'syncs' (5 chars) and 'data' to 'datas' (5 chars) in both beliefs would yield 2 qualifying terms and trigger conflict detection"

  - name: contradiction-preference-user-mediated
    description: >
      preferences.* contradictions are surfaced to the user for decision
      rather than auto-resolved, preserving user autonomy.
    preconditions:
      - "Belief A: 'User prefers dark theme with high contrast'"
      - "Belief B: 'User prefers light theme for daytime work'"
      - "Category: 'preferences.visual'"
    steps:
      - "Dispatch selects preferenceResolver for 'preferences' prefix"
      - "preferenceResolver packages both beliefs for user presentation"
    verify:
      - "Result action is USER_MEDIATED (or equivalent user-facing action)"
      - "Both beliefs included in result for user to choose between"
      - "Explanation event has SYSTEM origin and >= 50 chars"
      - "No automatic resolution occurs — user decision required"

  - name: contradiction-event-minimum-length
    description: >
      All explanation events generated by resolvers must be >= 50 characters
      with SYSTEM origin, regardless of which resolver produced them.
    preconditions:
      - "Contradictions prepared for each of the 4 resolver types"
    steps:
      - "Trigger resolution for constraints.*, preferences.*, models.*, heuristics.* categories"
      - "Collect explanation events from each resolution"
    verify:
      - "Every explanation event statement is >= 50 characters"
      - "Every explanation event origin is 'SYSTEM'"
      - "No event has null or empty statement"
      - "Short internal descriptions are padded to meet the 50-char minimum"

  # ── BootstrapEvents: Idempotent Initialization ──

  - name: bootstrap-sha256-deterministic-ids
    description: >
      Bootstrap event IDs are SHA-256 hashes of content, ensuring identical
      content always produces the same ID across runs.
    preconditions:
      - "CLAUDE.md and MEMORY.md exist with known content"
      - "PostgreSQL events table is empty"
    steps:
      - "Run BootstrapEvents for the first time"
      - "Events are generated from CLAUDE.md and MEMORY.md content"
      - "Each event ID is SHA-256(content)"
      - "Record all generated event IDs"
      - "Run BootstrapEvents a second time with identical files"
    verify:
      - "Event IDs from second run are identical to first run"
      - "SHA-256 hashes are deterministic — same content = same ID"
      - "No duplicate rows in events table (ON CONFLICT DO NOTHING)"
      - "Second run completes without errors despite all events being duplicates"

  - name: bootstrap-idempotent-rerun
    description: >
      Running BootstrapEvents multiple times is safe — ON CONFLICT DO NOTHING
      prevents duplicate entries while new content is still ingested.
    preconditions:
      - "BootstrapEvents has already run once successfully"
      - "Events table contains all previously bootstrapped events"
    steps:
      - "Run BootstrapEvents again without any file changes"
      - "All INSERT statements hit ON CONFLICT DO NOTHING"
      - "No exceptions thrown, no duplicate rows created"
      - "Add a new line to MEMORY.md"
      - "Run BootstrapEvents a third time"
    verify:
      - "Event count after second run equals first run (no duplicates)"
      - "Event count after third run equals first run + 1 (only new content added)"
      - "No integrity constraint violations"
      - "Process completes cleanly each time"

  - name: bootstrap-content-max-1500-chars
    description: >
      Event content from CLAUDE.md/MEMORY.md is capped at 1500 characters.
      Oversized sections are truncated, not rejected. Tests the exact
      boundary at 1500 and 1501 characters.
    preconditions:
      - "CLAUDE.md contains a section with >1500 characters of continuous content"
    steps:
      - "BootstrapEvents processes content of exactly 1500 characters"
      - "BootstrapEvents processes content of exactly 1501 characters"
      - "BootstrapEvents processes content of 3000 characters"
      - "SHA-256 IDs are computed for each"
    verify:
      - "Content at exactly 1500 chars is stored as-is (no truncation applied)"
      - "Content at exactly 1501 chars is truncated to 1500 chars"
      - "Content at 3000 chars is truncated to 1500 chars"
      - "Truncated content preserves the first 1500 chars (prefix, not random selection)"
      - "Event ID for 1500-char content is SHA-256 of the full 1500 chars"
      - "Event ID for 1501-char content is SHA-256 of the truncated 1500 chars (not original 1501)"
      - "No error or warning for oversized content — silent truncation"

  - name: bootstrap-fact-minimum-30-chars
    description: >
      Session key_facts below 30 characters are rejected during backfill
      as too terse to be meaningful.
    preconditions:
      - "Session exists with key_facts including: 'Uses JDK 25' (12 chars), 'PostgreSQL replaced SQLite for all persistent storage in gateway' (62 chars)"
    steps:
      - "BootstrapEvents backfill processes the session's key_facts"
      - "Fact 'Uses JDK 25' is checked against 30-char minimum"
      - "Fact is rejected (too short)"
      - "Longer fact passes the check and is processed"
    verify:
      - "Short fact (<30 chars) produces no event in the database"
      - "Long fact (>=30 chars) is converted to an event"
      - "Event statement for accepted fact is >= 50 chars (padded if needed)"
      - "No exception thrown for rejected facts — silent skip"

  - name: bootstrap-event-statement-padding-to-50
    description: >
      Event statements must be >= 50 characters. Facts between 30-49 chars
      are accepted but padded to meet the event minimum.
    preconditions:
      - "Session key_fact: 'FTS5 queries must be sanitized' (30 chars exactly)"
    steps:
      - "Fact passes 30-char minimum check"
      - "Converted to event statement"
      - "Statement is < 50 chars, so padding is applied"
      - "Padded statement is inserted as event"
    verify:
      - "Stored event statement is >= 50 characters"
      - "Original content is preserved (padding appended, not replacing)"
      - "SHA-256 ID is computed from the padded statement"

  - name: bootstrap-backfill-guard-tag-check
    description: >
      Sessions that have already been backfilled are detected via a tag
      and skipped, preventing duplicate event generation from key_facts.
    preconditions:
      - "Session S1 has key_facts and has been backfilled (tag present)"
      - "Session S2 has key_facts and has NOT been backfilled (no tag)"
    steps:
      - "BootstrapEvents backfill scans all sessions"
      - "S1 tag check returns true — session is skipped"
      - "S2 tag check returns false — session is processed"
      - "After processing S2, backfill tag is applied to S2"
    verify:
      - "No events generated from S1's key_facts (already backfilled)"
      - "Events generated from S2's key_facts"
      - "S2 now has backfill tag for future runs"
      - "Running backfill again skips both S1 and S2"

  - name: bootstrap-somatic-name-normalization
    description: >
      Historical content containing the old name 'somatic' is detected
      and normalized to 'sensor' during bootstrap event creation.
    preconditions:
      - "MEMORY.md or session key_fact contains text referencing 'somatic daemon' or 'somatic binary'"
    steps:
      - "BootstrapEvents processes content containing 'somatic'"
      - "Old name detection triggers normalization"
      - "'somatic' is replaced with 'sensor' in event content"
      - "SHA-256 ID computed from normalized content"
    verify:
      - "No event in database contains the word 'somatic'"
      - "Normalized events reference 'sensor' instead"
      - "Event ID is based on normalized (post-replacement) content"
      - "Original file content is NOT modified — only event content is normalized"

  - name: bootstrap-rate-limit-llm-retagging
    description: >
      LLM re-tagging during bootstrap is rate-limited to 50ms between calls
      to avoid overwhelming the model endpoint.
    preconditions:
      - "Multiple session key_facts require LLM re-tagging"
      - "LLM endpoint is available"
    steps:
      - "BootstrapEvents processes 10 key_facts requiring re-tagging"
      - "Each re-tag call is separated by >= 50ms delay"
      - "Total processing time for 10 facts >= 450ms (9 gaps * 50ms)"
    verify:
      - "Inter-call spacing is >= 50ms (measured via timestamps)"
      - "No rate limit errors from LLM endpoint"
      - "All 10 facts are re-tagged successfully"
      - "Rate limiting does not cause timeouts in the overall bootstrap process"

  # ── EventEmitter: Fire-and-Forget Emission ──

  - name: emitter-dual-write-postgres-primary
    description: >
      EventEmitter writes to PostgreSQL as primary store. The write must
      succeed or the event is considered lost.
    preconditions:
      - "PostgreSQL is available and events table exists"
      - "Kafka is available"
    steps:
      - "EventEmitter.emit() called with a valid event"
      - "Virtual thread spawned for the write"
      - "PostgreSQL INSERT executes first"
      - "Kafka publish follows"
      - "Both succeed"
    verify:
      - "Event exists in PostgreSQL events table"
      - "Event exists in Kafka topic"
      - "Event has SYSTEM origin"
      - "Caller was not blocked (fire-and-forget)"

  - name: emitter-kafka-failure-nonfatal
    description: >
      When Kafka is unavailable, the event is still persisted in PostgreSQL.
      Kafka failure is logged but never propagated to the caller.
    preconditions:
      - "PostgreSQL is available"
      - "Kafka is down or unreachable"
    steps:
      - "EventEmitter.emit() called with a valid event"
      - "PostgreSQL write succeeds"
      - "Kafka publish throws exception"
      - "Exception is caught and logged"
    verify:
      - "Event is persisted in PostgreSQL"
      - "No exception propagated to caller"
      - "Kafka failure is logged at WARN or ERROR level"
      - "Caller thread is not affected"

  - name: emitter-postgres-failure-event-lost
    description: >
      When PostgreSQL is unavailable, the event cannot be persisted.
      The failure is logged but not propagated — fire-and-forget contract.
    preconditions:
      - "PostgreSQL is down or unreachable"
      - "Kafka may or may not be available"
    steps:
      - "EventEmitter.emit() called with a valid event"
      - "PostgreSQL write throws exception"
      - "Exception is caught and logged"
      - "Kafka write may or may not be attempted (implementation-dependent)"
    verify:
      - "No exception propagated to caller"
      - "Event is NOT in PostgreSQL (write failed)"
      - "Failure is logged with sufficient detail for debugging"
      - "Caller continues unaffected — fire-and-forget contract honored"

  - name: emitter-virtual-thread-lifecycle
    description: >
      Each emit() spawns a virtual thread that completes independently.
      Virtual threads are not pooled — each emission gets a fresh thread.
      Measured with a constrained carrier pool to detect pinning.
    preconditions:
      - "PostgreSQL is available"
      - "JDK 25 runtime with virtual thread support"
      - "JVM started with -Djdk.tracePinnedThreads=full"
      - "Carrier thread pool size is 2 (-Djdk.virtualThreadScheduler.parallelism=2)"
    steps:
      - "Call EventEmitter.emit() 100 times in rapid succession"
      - "Each call spawns a virtual thread"
      - "Virtual threads execute concurrently on 2 carrier threads"
      - "Monitor stderr for jdk.tracePinnedThreads warnings"
      - "Record wall-clock time from first emit() to last event persisted"
      - "All threads complete and are garbage collected"
    verify:
      - "All 100 events are persisted in PostgreSQL"
      - "Zero carrier pinning warnings in stderr (no synchronized blocks in emit path)"
      - "100 events complete within 5000ms with carrier pool size of 2"
      - "Caller returns immediately for each emit() call (< 10ms per emit)"
      - "No thread pool exhaustion — virtual threads are lightweight"

  - name: emitter-system-origin-enforced
    description: >
      EventEmitter always sets SYSTEM origin on emitted events, regardless
      of what the caller provides.
    preconditions:
      - "PostgreSQL is available"
    steps:
      - "Call emit() with an event that has origin set to 'user'"
      - "EventEmitter overrides origin to 'SYSTEM'"
      - "Event is persisted"
    verify:
      - "Persisted event has origin = 'SYSTEM'"
      - "Caller-provided origin is ignored/overridden"
      - "All events from EventEmitter are indistinguishable as SYSTEM-originated"

  - name: emitter-backpressure-under-burst
    description: >
      Under extreme burst (10,000 concurrent emit() calls), all events
      eventually persist and emit() remains non-blocking for the caller
      even when the connection pool (5 connections) is saturated.
    preconditions:
      - "PostgreSQL is available with a 5-connection pool (HikariCP or equivalent)"
      - "Kafka is available"
      - "JDK 25 runtime with virtual thread support"
    steps:
      - "Spawn 10,000 virtual threads, each calling EventEmitter.emit() simultaneously"
      - "Record wall-clock time for each emit() call (time from call to return)"
      - "Monitor connection pool: active connections, pending acquisitions, timeouts"
      - "Wait for all virtual threads to complete (with a generous timeout, e.g., 60s)"
      - "Count events persisted in PostgreSQL"
    verify:
      - "All 10,000 events are eventually persisted in PostgreSQL (zero loss)"
      - "Every emit() call returns within 10ms (non-blocking for caller)"
      - "Connection pool may saturate (5 active connections) but virtual threads yield, not block carriers"
      - "No ConnectionPoolExhausted or timeout exceptions propagated to callers"
      - "Total wall-clock time for all 10,000 events to persist is bounded (< 60s with 5 connections)"
      - "Fire-and-forget contract: callers are decoupled from persistence latency"

  - name: emitter-nonblocking-caller-verification
    description: >
      The caller of emit() must never be blocked, even when PostgreSQL
      or Kafka writes are slow.
    preconditions:
      - "PostgreSQL is available but responding slowly (simulated 2s latency)"
      - "Kafka is available but responding slowly"
    steps:
      - "Record timestamp T0"
      - "Call EventEmitter.emit()"
      - "Record timestamp T1 immediately after emit() returns"
      - "Wait for virtual thread to complete"
      - "Verify event is eventually persisted"
    verify:
      - "T1 - T0 < 10ms (caller not blocked by DB latency)"
      - "Event eventually appears in PostgreSQL (after virtual thread completes)"
      - "No caller-visible delay regardless of backend latency"

  - name: bootstrap-postgres-unavailable
    description: >
      When PostgreSQL is unreachable during gateway startup, the bootstrap
      process must not prevent the gateway from starting. Bootstrap is
      best-effort — failure is logged and retried on next restart.
    preconditions:
      - "Gateway is configured with PostgreSQL connection details"
      - "PostgreSQL is down or unreachable (connection refused, timeout, etc.)"
      - "CLAUDE.md and MEMORY.md exist with valid content"
    steps:
      - "Start the gateway"
      - "BootstrapEvents attempts to connect to PostgreSQL"
      - "Connection fails (timeout or refused)"
      - "BootstrapEvents logs a WARNING with connection details"
      - "Gateway continues startup (HTTP daemon, MCP tools, display management)"
      - "Gateway reaches healthy state and serves requests"
      - "PostgreSQL comes back online"
      - "Gateway is restarted"
      - "BootstrapEvents runs again and succeeds"
    verify:
      - "Gateway startup is not blocked by PostgreSQL unavailability"
      - "WARNING (not ERROR) is logged for bootstrap failure with connection details"
      - "All non-PostgreSQL gateway functionality works (MCP tools, display, sensor proxy)"
      - "No uncaught exception propagates from bootstrap to crash the JVM"
      - "On next restart with PostgreSQL available, bootstrap completes successfully"
      - "Events from the successful bootstrap are identical to what would have been created on first attempt"
      - "Idempotency holds — partial bootstrap followed by full bootstrap produces correct state"

  # ── TurnCapture: Conversation Turn Recording ──

  - name: turncapture-500-char-truncation
    description: >
      Content exceeding MAX_CONTENT_LENGTH (500 chars) is truncated.
      Exactly 500 chars are stored, no more.
    preconditions:
      - "PostgreSQL is available"
      - "Turn content is 1000 characters long"
    steps:
      - "TurnCapture.capture() called with 1000-char content"
      - "Content is truncated to 500 characters"
      - "Truncated content is stored in PostgreSQL"
    verify:
      - "Stored content is exactly 500 characters"
      - "First 500 chars of original content are preserved"
      - "No error for oversized content — silent truncation"
      - "Content at exactly 500 chars is stored as-is (no truncation)"
      - "Content at 501 chars is truncated to 500"

  - name: turncapture-uuid-uniqueness
    description: >
      Each captured turn gets a unique UUID. No two turns share an ID,
      even when captured in rapid succession.
    preconditions:
      - "PostgreSQL is available"
    steps:
      - "Capture 1000 turns in rapid succession with identical content"
      - "Each turn is assigned a UUID"
      - "All turns are inserted into PostgreSQL"
    verify:
      - "All 1000 UUIDs are distinct"
      - "No primary key violations"
      - "UUID format is standard UUID v4 (8-4-4-4-12 hex)"
      - "Identical content does NOT produce identical UUIDs (unlike BootstrapEvents' SHA-256)"

  - name: turncapture-role-validation
    description: >
      Turn roles are restricted to user, agent, and system.
      Invalid roles should be handled gracefully.
    preconditions:
      - "PostgreSQL is available"
    steps:
      - "Capture turn with role='user' — succeeds"
      - "Capture turn with role='agent' — succeeds"
      - "Capture turn with role='system' — succeeds"
      - "Capture turn with role='admin' — behavior observed"
    verify:
      - "Turns with valid roles (user, agent, system) are persisted"
      - "Invalid role handling is defined (either rejected or stored as-is)"
      - "Role field is stored exactly as provided for valid values"

  - name: turncapture-tool-name-quote-escaping
    description: >
      Tool names containing double quotes have them replaced with single
      quotes to prevent JSON/SQL injection in stored metadata.
    preconditions:
      - "PostgreSQL is available"
      - "Turn includes tool_name with embedded double quotes"
    steps:
      - "Capture turn with tool_name='mcp__gateway__SENSE_read_\"buffer\"'"
      - "TurnCapture escapes double quotes to single quotes"
      - "Stored tool_name becomes 'mcp__gateway__SENSE_read_'buffer''"
    verify:
      - "No double quotes in stored tool_name"
      - "Single quotes replace double quotes exactly"
      - "Tool name without quotes is stored unchanged"
      - "Empty tool name is stored as empty (not null-coerced)"

  - name: turncapture-iso8601-timestamps
    description: >
      All turn timestamps are stored in ISO-8601 format for consistent
      parsing across systems.
    preconditions:
      - "PostgreSQL is available"
    steps:
      - "Capture a turn at a known time"
      - "Read back the stored timestamp"
    verify:
      - "Timestamp matches ISO-8601 format (yyyy-MM-dd'T'HH:mm:ss.SSSZ or equivalent)"
      - "Timestamp is in UTC (Z suffix or +00:00)"
      - "Timestamp precision includes at least seconds"
      - "Timestamp is within a few seconds of capture time (not stale or default)"

  - name: turncapture-dual-write-kafka-optional
    description: >
      TurnCapture follows the same dual-write pattern as EventEmitter —
      PostgreSQL is required, Kafka is optional.
    preconditions:
      - "PostgreSQL is available"
      - "Kafka is down"
    steps:
      - "Capture a turn"
      - "PostgreSQL write succeeds"
      - "Kafka publish fails"
      - "Failure is logged"
    verify:
      - "Turn is persisted in PostgreSQL"
      - "Kafka failure does not prevent turn storage"
      - "Kafka failure is logged but not propagated"
      - "Caller is not blocked or notified of Kafka failure"

  - name: turncapture-empty-and-null-content
    description: >
      Edge case handling for empty string and null content in turns.
    preconditions:
      - "PostgreSQL is available"
    steps:
      - "Capture turn with content='' (empty string)"
      - "Capture turn with content=null"
      - "Observe behavior for each"
    verify:
      - "Empty string is stored as empty (not rejected, not null-coerced)"
      - "Null content handling is defined (either stored as null or rejected with error)"
      - "Truncation logic handles empty/null without exception"
      - "UUID and timestamp are still generated regardless of content"

  - name: turncapture-concurrent-capture-integrity
    description: >
      Multiple concurrent turn captures do not corrupt each other or
      produce duplicate UUIDs.
    preconditions:
      - "PostgreSQL is available with sufficient connection pool"
    steps:
      - "Spawn 50 concurrent threads, each capturing a turn with distinct content"
      - "All threads execute capture simultaneously"
      - "All writes complete"
    verify:
      - "Exactly 50 turns in the database"
      - "All 50 UUIDs are unique"
      - "Each turn's content matches what was submitted (no cross-contamination)"
      - "Timestamps reflect actual capture time (not serialized/queued time)"
      - "No deadlocks or connection pool exhaustion"
