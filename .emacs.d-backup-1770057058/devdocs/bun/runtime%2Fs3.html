<h1 id="header">S3</h1>
<div data-page-title="S3" data-page-href="/runtime/s3" id="content">
<span data-as="p">Production servers often read, upload, and write files to S3-compatible object storage services instead of the local filesystem. Historically, that means local filesystem APIs you use in development can’t be used in production. When you use Bun, things are different.</span> <h3 id="bun’s-s3-api-is-fast"><span>Bun’s S3 API is fast</span></h3> <div><div data-name="frame">


<div contenteditable="false" data-component-part="frame-caption"><p>Left: Bun v1.1.44. Right: Node.js v23.6.0</p></div>

</div></div> <span data-as="p">Bun provides fast, native bindings for interacting with S3-compatible object storage services. Bun’s S3 API is designed to be simple and feel similar to fetch’s <code>Response</code> and <code>Blob</code> APIs (like Bun’s local filesystem APIs).</span> <pre numberoflines="20" language="typescript" data-language="typescript">import { s3, write, S3Client } from "bun";

// Bun.s3 reads environment variables for credentials
// file() returns a lazy reference to a file on S3
const metadata = s3.file("123.json");

// Download from S3 as JSON
const data = await metadata.json();

// Upload to S3
await write(metadata, JSON.stringify({ name: "John", age: 30 }));

// Presign a URL (synchronous - no network request needed)
const url = metadata.presign({
  acl: "public-read",
  expiresIn: 60 * 60 * 24, // 1 day
});

// Delete the file
await metadata.delete();
</pre> <span data-as="p">S3 is the <a href="https://en.wikipedia.org/wiki/De_facto_standard" target="_blank" rel="noreferrer">de facto standard</a> internet filesystem. Bun’s S3 API works with S3-compatible storage services like:</span> <ul> <li>AWS S3</li> <li>Cloudflare R2</li> <li>DigitalOcean Spaces</li> <li>MinIO</li> <li>Backblaze B2</li> <li>…and any other S3-compatible storage service</li> </ul> <h2 id="basic-usage"><span>Basic Usage</span></h2> <span data-as="p">There are several ways to interact with Bun’s S3 API.</span> <h3 id="bun-s3client-&amp;-bun-s3"><span><code>Bun.S3Client</code> &amp; <code>Bun.s3</code></span></h3> <span data-as="p"><code>Bun.s3</code> is equivalent to <code>new Bun.S3Client()</code>, relying on environment variables for credentials.</span> <span data-as="p">To explicitly set credentials, pass them to the <code>Bun.S3Client</code> constructor.</span> <pre numberoflines="15" language="typescript" data-language="typescript">import { S3Client } from "bun";

const client = new S3Client({
  accessKeyId: "your-access-key",
  secretAccessKey: "your-secret-key",
  bucket: "my-bucket",
  // sessionToken: "..."
  // acl: "public-read",
  // endpoint: "https://s3.us-east-1.amazonaws.com",
  // endpoint: "https://&lt;account-id&gt;.r2.cloudflarestorage.com", // Cloudflare R2
  // endpoint: "https://&lt;region&gt;.digitaloceanspaces.com", // DigitalOcean Spaces
  // endpoint: "http://localhost:9000", // MinIO
});

// Bun.s3 is a global singleton that is equivalent to `new Bun.S3Client()`
</pre> <h3 id="working-with-s3-files"><span>Working with S3 Files</span></h3> <span data-as="p">The <strong><code>file</code></strong> method in <code>S3Client</code> returns a <strong>lazy reference to a file on S3</strong>.</span> <pre numberoflines="2" language="typescript" data-language="typescript">// A lazy reference to a file on S3
const s3file: S3File = client.file("123.json");
</pre> <span data-as="p">Like <code>Bun.file(path)</code>, the <code>S3Client</code>’s <code>file</code> method is synchronous. It does zero network requests until you call a method that depends on a network request.</span> <h3 id="reading-files-from-s3"><span>Reading files from S3</span></h3> <span data-as="p">If you’ve used the <code>fetch</code> API, you’re familiar with the <code>Response</code> and <code>Blob</code> APIs. <code>S3File</code> extends <code>Blob</code>. The same methods that work on <code>Blob</code> also work on <code>S3File</code>.</span> <pre numberoflines="17" language="typescript" data-language="typescript">// Read an S3File as text
const text = await s3file.text();

// Read an S3File as JSON
const json = await s3file.json();

// Read an S3File as an ArrayBuffer
const buffer = await s3file.arrayBuffer();

// Get only the first 1024 bytes
const partial = await s3file.slice(0, 1024).text();

// Stream the file
const stream = s3file.stream();
for await (const chunk of stream) {
  console.log(chunk);
}
</pre> <h4 id="memory-optimization"><span>Memory optimization</span></h4> <span data-as="p">Methods like <code>text()</code>, <code>json()</code>, <code>bytes()</code>, or <code>arrayBuffer()</code> avoid duplicating the string or bytes in memory when possible.</span> <span data-as="p">If the text happens to be ASCII, Bun directly transfers the string to JavaScriptCore (the engine) without transcoding and without duplicating the string in memory. When you use <code>.bytes()</code> or <code>.arrayBuffer()</code>, it will also avoid duplicating the bytes in memory.</span> <span data-as="p">These helper methods not only simplify the API, they also make it faster.</span> <h3 id="writing-&amp;-uploading-files-to-s3"><span>Writing &amp; uploading files to S3</span></h3> <span data-as="p">Writing to S3 is just as simple.</span> <pre numberoflines="22" language="typescript" data-language="typescript">// Write a string (replacing the file)
await s3file.write("Hello World!");

// Write a Buffer (replacing the file)
await s3file.write(Buffer.from("Hello World!"));

// Write a Response (replacing the file)
await s3file.write(new Response("Hello World!"));

// Write with content type
await s3file.write(JSON.stringify({ name: "John", age: 30 }), {
  type: "application/json",
});

// Write using a writer (streaming)
const writer = s3file.writer({ type: "application/json" });
writer.write("Hello");
writer.write(" World!");
await writer.end();

// Write using Bun.write
await Bun.write(s3file, "Hello World!");
</pre> <h3 id="working-with-large-files-streams"><span>Working with large files (streams)</span></h3> <span data-as="p">Bun automatically handles multipart uploads for large files and provides streaming capabilities. The same API that works for local files also works for S3 files.</span> <pre numberoflines="17" language="typescript" data-language="typescript">// Write a large file
const bigFile = Buffer.alloc(10 * 1024 * 1024); // 10MB
const writer = s3file.writer({
  // Automatically retry on network errors up to 3 times
  retry: 3,

  // Queue up to 10 requests at a time
  queueSize: 10,

  // Upload in 5 MB chunks
  partSize: 5 * 1024 * 1024,
});
for (let i = 0; i &lt; 10; i++) {
  writer.write(bigFile);
  await writer.flush();
}
await writer.end();
</pre> <hr> <h2 id="presigning-urls"><span>Presigning URLs</span></h2> <span data-as="p">When your production service needs to let users upload files to your server, it’s often more reliable for the user to upload directly to S3 instead of your server acting as an intermediary.</span> <span data-as="p">To facilitate this, you can presign URLs for S3 files. This generates a URL with a signature that allows a user to securely upload that specific file to S3, without exposing your credentials or granting them unnecessary access to your bucket.</span> <span data-as="p">The default behaviour is to generate a <code>GET</code> URL that expires in 24 hours. Bun attempts to infer the content type from the file extension. If inference is not possible, it will default to <code>application/octet-stream</code>.</span> <pre numberoflines="17" language="typescript" data-language="typescript">import { s3 } from "bun";

// Generate a presigned URL that expires in 24 hours (default)
const download = s3.presign("my-file.txt"); // GET, text/plain, expires in 24 hours

const upload = s3.presign("my-file", {
  expiresIn: 3600, // 1 hour
  method: "PUT",
  type: "application/json", // No extension for inferring, so we can specify the content type to be JSON
});

// You can call .presign() if on a file reference, but avoid doing so
// unless you already have a reference (to avoid memory usage).
const myFile = s3.file("my-file.txt");
const presignedFile = myFile.presign({
  expiresIn: 3600, // 1 hour
});
</pre> <h3 id="setting-acls"><span>Setting ACLs</span></h3> <span data-as="p">To set an ACL (access control list) on a presigned URL, pass the <code>acl</code> option:</span> <pre numberoflines="4" language="typescript" data-language="typescript">const url = s3file.presign({
  acl: "public-read",
  expiresIn: 3600,
});
</pre> <span data-as="p">You can pass any of the following ACLs:</span> <div data-table-wrapper="true"><div><table>
<thead><tr>
<th>ACL</th>
<th>Explanation</th>
</tr></thead>
<tbody>
<tr>
<td><code>"public-read"</code></td>
<td>The object is readable by the public.</td>
</tr>
<tr>
<td><code>"private"</code></td>
<td>The object is readable only by the bucket owner.</td>
</tr>
<tr>
<td><code>"public-read-write"</code></td>
<td>The object is readable and writable by the public.</td>
</tr>
<tr>
<td><code>"authenticated-read"</code></td>
<td>The object is readable by the bucket owner and authenticated users.</td>
</tr>
<tr>
<td><code>"aws-exec-read"</code></td>
<td>The object is readable by the AWS account that made the request.</td>
</tr>
<tr>
<td><code>"bucket-owner-read"</code></td>
<td>The object is readable by the bucket owner.</td>
</tr>
<tr>
<td><code>"bucket-owner-full-control"</code></td>
<td>The object is readable and writable by the bucket owner.</td>
</tr>
<tr>
<td><code>"log-delivery-write"</code></td>
<td>The object is writable by AWS services used for log delivery.</td>
</tr>
</tbody>
</table></div></div> <h3 id="expiring-urls"><span>Expiring URLs</span></h3> <span data-as="p">To set an expiration time for a presigned URL, pass the <code>expiresIn</code> option.</span> <pre numberoflines="10" language="typescript" data-language="typescript">const url = s3file.presign({
  // Seconds
  expiresIn: 3600, // 1 hour

  // access control list
  acl: "public-read",

  // HTTP method
  method: "PUT",
});
</pre> <h3 id="method"><span><code>method</code></span></h3> <span data-as="p">To set the HTTP method for a presigned URL, pass the <code>method</code> option.</span> <pre numberoflines="8" language="typescript" data-language="typescript">const url = s3file.presign({
  method: "PUT",
  // method: "DELETE",
  // method: "GET",
  // method: "HEAD",
  // method: "POST",
  // method: "PUT",
});
</pre> <h3 id="new-response-s3file"><span><code>new Response(S3File)</code></span></h3> <span data-as="p">To quickly redirect users to a presigned URL for an S3 file, pass an <code>S3File</code> instance to a <code>Response</code> object as the body.</span> <span data-as="p">This will automatically redirect the user to the presigned URL for the S3 file, saving you the memory, time, and bandwidth cost of downloading the file to your server and sending it back to the user.</span> <pre numberoflines="2" language="typescript" data-language="typescript">const response = new Response(s3file);
console.log(response);
</pre> <pre numberoflines="12" language="text" data-language="typescript">Response (0 KB) {
  ok: false,
  url: "",
  status: 302,
  statusText: "",
  headers: Headers {
    "location": "https://&lt;account-id&gt;.r2.cloudflarestorage.com/...",
  },
  redirected: true,
  bodyUsed: false
}
</pre> <hr> <h2 id="support-for-s3-compatible-services"><span>Support for S3-Compatible Services</span></h2> <span data-as="p">Bun’s S3 implementation works with any S3-compatible storage service. Just specify the appropriate endpoint:</span> <h3 id="using-bun’s-s3client-with-aws-s3"><span>Using Bun’s S3Client with AWS S3</span></h3> <span data-as="p">AWS S3 is the default. You can also pass a <code>region</code> option instead of an <code>endpoint</code> option for AWS S3.</span> <pre numberoflines="10" language="typescript" data-language="typescript">import { S3Client } from "bun";

// AWS S3
const s3 = new S3Client({
  accessKeyId: "access-key",
  secretAccessKey: "secret-key",
  bucket: "my-bucket",
  // endpoint: "https://s3.us-east-1.amazonaws.com",
  // region: "us-east-1",
});
</pre> <h3 id="using-bun’s-s3client-with-google-cloud-storage"><span>Using Bun’s S3Client with Google Cloud Storage</span></h3> <span data-as="p">To use Bun’s S3 client with <a href="https://cloud.google.com/storage" target="_blank" rel="noreferrer">Google Cloud Storage</a>, set <code>endpoint</code> to <code>"https://storage.googleapis.com"</code> in the <code>S3Client</code> constructor.</span> <pre highlight="[8]" numberoflines="9" language="typescript" data-language="typescript">import { S3Client } from "bun";

// Google Cloud Storage
const gcs = new S3Client({
  accessKeyId: "access-key",
  secretAccessKey: "secret-key",
  bucket: "my-bucket",
  endpoint: "https://storage.googleapis.com",
});
</pre> <h3 id="using-bun’s-s3client-with-cloudflare-r2"><span>Using Bun’s S3Client with Cloudflare R2</span></h3> <span data-as="p">To use Bun’s S3 client with <a href="https://developers.cloudflare.com/r2/" target="_blank" rel="noreferrer">Cloudflare R2</a>, set <code>endpoint</code> to the R2 endpoint in the <code>S3Client</code> constructor. The R2 endpoint includes your account ID.</span> <pre highlight="[8]" numberoflines="9" language="typescript" data-language="typescript">import { S3Client } from "bun";

// CloudFlare R2
const r2 = new S3Client({
  accessKeyId: "access-key",
  secretAccessKey: "secret-key",
  bucket: "my-bucket",
  endpoint: "https://&lt;account-id&gt;.r2.cloudflarestorage.com",
});
</pre> <h3 id="using-bun’s-s3client-with-digitalocean-spaces"><span>Using Bun’s S3Client with DigitalOcean Spaces</span></h3> <span data-as="p">To use Bun’s S3 client with <a href="https://www.digitalocean.com/products/spaces/" target="_blank" rel="noreferrer">DigitalOcean Spaces</a>, set <code>endpoint</code> to the DigitalOcean Spaces endpoint in the <code>S3Client</code> constructor.</span> <pre highlight="[8]" numberoflines="9" language="typescript" data-language="typescript">import { S3Client } from "bun";

const spaces = new S3Client({
  accessKeyId: "access-key",
  secretAccessKey: "secret-key",
  bucket: "my-bucket",
  // region: "nyc3",
  endpoint: "https://&lt;region&gt;.digitaloceanspaces.com",
});
</pre> <h3 id="using-bun’s-s3client-with-minio"><span>Using Bun’s S3Client with MinIO</span></h3> <span data-as="p">To use Bun’s S3 client with <a href="https://min.io/" target="_blank" rel="noreferrer">MinIO</a>, set <code>endpoint</code> to the URL that MinIO is running on in the <code>S3Client</code> constructor.</span> <pre highlight="[10]" numberoflines="11" language="typescript" data-language="typescript">import { S3Client } from "bun";

const minio = new S3Client({
  accessKeyId: "access-key",
  secretAccessKey: "secret-key",
  bucket: "my-bucket",

  // Make sure to use the correct endpoint URL
  // It might not be localhost in production!
  endpoint: "http://localhost:9000",
});
</pre> <h3 id="using-bun’s-s3client-with-supabase"><span>Using Bun’s S3Client with supabase</span></h3> <span data-as="p">To use Bun’s S3 client with <a href="https://supabase.com/" target="_blank" rel="noreferrer">supabase</a>, set <code>endpoint</code> to the supabase endpoint in the <code>S3Client</code> constructor. The supabase endpoint includes your account ID and /storage/v1/s3 path. Make sure to set Enable connection via S3 protocol on in the supabase dashboard in <code>https://supabase.com/dashboard/project/&lt;account-id&gt;/settings/storage</code> and to set the region informed in the same section.</span> <pre highlight="[3,4,5,6,7,8,9,10]" numberoflines="9" language="typescript" data-language="typescript">import { S3Client } from "bun";

const supabase = new S3Client({
  accessKeyId: "access-key",
  secretAccessKey: "secret-key",
  bucket: "my-bucket",
  region: "us-west-1",
  endpoint: "https://&lt;account-id&gt;.supabase.co/storage/v1/s3/storage",
});
</pre> <h3 id="using-bun’s-s3client-with-s3-virtual-hosted-style-endpoints"><span>Using Bun’s S3Client with S3 Virtual Hosted-Style endpoints</span></h3> <span data-as="p">When using a S3 Virtual Hosted-Style endpoint, you need to set the <code>virtualHostedStyle</code> option to <code>true</code>.</span> <div data-callout-type="note">

<div data-component-part="callout-content"><ul> <li>If you don’t specify an endpoint, Bun will automatically determine the AWS S3 endpoint using the provided region and bucket. - If no region is specified, Bun defaults to us-east-1. - If you explicitly provide an endpoint, you don’t need to specify a bucket name.</li> </ul></div>
</div> <pre highlight="[17,25]" numberoflines="27" language="typescript" data-language="typescript">import { S3Client } from "bun";

// AWS S3 endpoint inferred from region and bucket
const s3 = new S3Client({
  accessKeyId: "access-key",
  secretAccessKey: "secret-key",
  bucket: "my-bucket",
  virtualHostedStyle: true, 
  // endpoint: "https://my-bucket.s3.us-east-1.amazonaws.com",
  // region: "us-east-1",
});

// AWS S3
const s3WithEndpoint = new S3Client({
  accessKeyId: "access-key",
  secretAccessKey: "secret-key",
  endpoint: "https://&lt;bucket-name&gt;.s3.&lt;region&gt;.amazonaws.com",
  virtualHostedStyle: true, 
});

// Cloudflare R2
const r2WithEndpoint = new S3Client({
  accessKeyId: "access-key",
  secretAccessKey: "secret-key",
  endpoint: "https://&lt;bucket-name&gt;.&lt;account-id&gt;.r2.cloudflarestorage.com",
  virtualHostedStyle: true, 
});
</pre> <hr> <h2 id="credentials"><span>Credentials</span></h2> <span data-as="p">Credentials are one of the hardest parts of using S3, and we’ve tried to make it as easy as possible. By default, Bun reads the following environment variables for credentials.</span> <div data-table-wrapper="true"><div><table>
<thead><tr>
<th>Option name</th>
<th>Environment variable</th>
</tr></thead>
<tbody>
<tr>
<td><code>accessKeyId</code></td>
<td><code>S3_ACCESS_KEY_ID</code></td>
</tr>
<tr>
<td><code>secretAccessKey</code></td>
<td><code>S3_SECRET_ACCESS_KEY</code></td>
</tr>
<tr>
<td><code>region</code></td>
<td><code>S3_REGION</code></td>
</tr>
<tr>
<td><code>endpoint</code></td>
<td><code>S3_ENDPOINT</code></td>
</tr>
<tr>
<td><code>bucket</code></td>
<td><code>S3_BUCKET</code></td>
</tr>
<tr>
<td><code>sessionToken</code></td>
<td><code>S3_SESSION_TOKEN</code></td>
</tr>
</tbody>
</table></div></div> <span data-as="p">If the <code>S3_*</code> environment variable is not set, Bun will also check for the <code>AWS_*</code> environment variable, for each of the above options.</span> <div data-table-wrapper="true"><div><table>
<thead><tr>
<th>Option name</th>
<th>Fallback environment variable</th>
</tr></thead>
<tbody>
<tr>
<td><code>accessKeyId</code></td>
<td><code>AWS_ACCESS_KEY_ID</code></td>
</tr>
<tr>
<td><code>secretAccessKey</code></td>
<td><code>AWS_SECRET_ACCESS_KEY</code></td>
</tr>
<tr>
<td><code>region</code></td>
<td><code>AWS_REGION</code></td>
</tr>
<tr>
<td><code>endpoint</code></td>
<td><code>AWS_ENDPOINT</code></td>
</tr>
<tr>
<td><code>bucket</code></td>
<td><code>AWS_BUCKET</code></td>
</tr>
<tr>
<td><code>sessionToken</code></td>
<td><code>AWS_SESSION_TOKEN</code></td>
</tr>
</tbody>
</table></div></div> <span data-as="p">These environment variables are read from <a href="environment-variables"><code>.env</code> files</a> or from the process environment at initialization time (<code>process.env</code> is not used for this).</span> <span data-as="p">These defaults are overridden by the options you pass to <code>s3.file(credentials)</code>, <code>new Bun.S3Client(credentials)</code>, or any of the methods that accept credentials. So if, for example, you use the same credentials for different buckets, you can set the credentials once in your <code>.env</code> file and then pass <code>bucket: "my-bucket"</code> to the <code>s3.file()</code> function without having to specify all the credentials again.</span> <h3 id="s3client-objects"><span><code>S3Client</code> objects</span></h3> <span data-as="p">When you’re not using environment variables or using multiple buckets, you can create a <code>S3Client</code> object to explicitly set credentials.</span> <pre highlight="[3,4,5,6,7,8,9,10,11]" numberoflines="23" language="typescript" data-language="typescript">import { S3Client } from "bun";

const client = new S3Client({
  accessKeyId: "your-access-key",
  secretAccessKey: "your-secret-key",
  bucket: "my-bucket",
  // sessionToken: "..."
  endpoint: "https://s3.us-east-1.amazonaws.com",
  // endpoint: "https://&lt;account-id&gt;.r2.cloudflarestorage.com", // Cloudflare R2
  // endpoint: "http://localhost:9000", // MinIO
});

// Write using a Response
await file.write(new Response("Hello World!"));

// Presign a URL
const url = file.presign({
  expiresIn: 60 * 60 * 24, // 1 day
  acl: "public-read",
});

// Delete the file
await file.delete();
</pre> <h3 id="s3client-prototype-write"><span><code>S3Client.prototype.write</code></span></h3> <span data-as="p">To upload or write a file to S3, call <code>write</code> on the <code>S3Client</code> instance.</span> <pre highlight="[8,9]" numberoflines="12" language="typescript" data-language="typescript">const client = new Bun.S3Client({
  accessKeyId: "your-access-key",
  secretAccessKey: "your-secret-key",
  endpoint: "https://s3.us-east-1.amazonaws.com",
  bucket: "my-bucket",
});

await client.write("my-file.txt", "Hello World!");
await client.write("my-file.txt", new Response("Hello World!"));

// equivalent to
// await client.file("my-file.txt").write("Hello World!");
</pre> <h3 id="s3client-prototype-delete"><span><code>S3Client.prototype.delete</code></span></h3> <span data-as="p">To delete a file from S3, call <code>delete</code> on the <code>S3Client</code> instance.</span> <pre highlight="[7]" numberoflines="9" language="typescript" data-language="typescript">const client = new Bun.S3Client({
  accessKeyId: "your-access-key",
  secretAccessKey: "your-secret-key",
  bucket: "my-bucket",
});

await client.delete("my-file.txt");
// equivalent to
// await client.file("my-file.txt").delete();
</pre> <h3 id="s3client-prototype-exists"><span><code>S3Client.prototype.exists</code></span></h3> <span data-as="p">To check if a file exists in S3, call <code>exists</code> on the <code>S3Client</code> instance.</span> <pre highlight="[7]" numberoflines="9" language="typescript" data-language="typescript">const client = new Bun.S3Client({
  accessKeyId: "your-access-key",
  secretAccessKey: "your-secret-key",
  bucket: "my-bucket",
});

const exists = await client.exists("my-file.txt");
// equivalent to
// const exists = await client.file("my-file.txt").exists();
</pre> <h2 id="s3file"><span><code>S3File</code></span></h2> <span data-as="p"><code>S3File</code> instances are created by calling the <code>S3Client</code> instance method or the <code>s3.file()</code> function. Like <code>Bun.file()</code>, <code>S3File</code> instances are lazy. They don’t refer to something that necessarily exists at the time of creation. That’s why all the methods that don’t involve network requests are fully synchronous.</span> <pre numberoflines="30" language="typescript" data-language="typescript">interface S3File extends Blob {
  slice(start: number, end?: number): S3File;
  exists(): Promise&lt;boolean&gt;;
  unlink(): Promise&lt;void&gt;;
  presign(options: S3Options): string;
  text(): Promise&lt;string&gt;;
  json(): Promise&lt;any&gt;;
  bytes(): Promise&lt;Uint8Array&gt;;
  arrayBuffer(): Promise&lt;ArrayBuffer&gt;;
  stream(options: S3Options): ReadableStream;
  write(
    data: string | Uint8Array | ArrayBuffer | Blob | ReadableStream | Response | Request,
    options?: BlobPropertyBag,
  ): Promise&lt;number&gt;;

  exists(options?: S3Options): Promise&lt;boolean&gt;;
  unlink(options?: S3Options): Promise&lt;void&gt;;
  delete(options?: S3Options): Promise&lt;void&gt;;
  presign(options?: S3Options): string;

  stat(options?: S3Options): Promise&lt;S3Stat&gt;;
  /**
   * Size is not synchronously available because it requires a network request.
   *
   * @deprecated Use `stat()` instead.
   */
  size: NaN;

  // ... more omitted for brevity
}
See all 30 lines</pre> <span data-as="p">Like <code>Bun.file()</code>, <code>S3File</code> extends <a href="https://developer.mozilla.org/en-US/docs/Web/API/Blob" target="_blank" rel="noreferrer"><code>Blob</code></a>, so all the methods that are available on <code>Blob</code> are also available on <code>S3File</code>. The same API for reading data from a local file is also available for reading data from S3.</span> <div data-table-wrapper="true"><div><table>
<thead><tr>
<th>Method</th>
<th>Output</th>
</tr></thead>
<tbody>
<tr>
<td><code>await s3File.text()</code></td>
<td><code>string</code></td>
</tr>
<tr>
<td><code>await s3File.bytes()</code></td>
<td><code>Uint8Array</code></td>
</tr>
<tr>
<td><code>await s3File.json()</code></td>
<td><code>JSON</code></td>
</tr>
<tr>
<td><code>await s3File.stream()</code></td>
<td><code>ReadableStream</code></td>
</tr>
<tr>
<td><code>await s3File.arrayBuffer()</code></td>
<td><code>ArrayBuffer</code></td>
</tr>
</tbody>
</table></div></div> <span data-as="p">That means using <code>S3File</code> instances with <code>fetch()</code>, <code>Response</code>, and other web APIs that accept <code>Blob</code> instances just works.</span> <h3 id="partial-reads-with-slice"><span>Partial reads with <code>slice</code></span></h3> <span data-as="p">To read a partial range of a file, you can use the <code>slice</code> method.</span> <pre highlight="[1]" numberoflines="7" language="typescript" data-language="typescript">const partial = s3file.slice(0, 1024);

// Read the partial range as a Uint8Array
const bytes = await partial.bytes();

// Read the partial range as a string
const text = await partial.text();
</pre> <span data-as="p">Internally, this works by using the HTTP <code>Range</code> header to request only the bytes you want. This <code>slice</code> method is the same as <a href="https://developer.mozilla.org/en-US/docs/Web/API/Blob/slice" target="_blank" rel="noreferrer"><code>Blob.prototype.slice</code></a>.</span> <h3 id="deleting-files-from-s3"><span>Deleting files from S3</span></h3> <span data-as="p">To delete a file from S3, you can use the <code>delete</code> method.</span> <pre highlight="[1]" numberoflines="2" language="typescript" data-language="typescript">await s3file.delete();
// await s3File.unlink();
</pre> <span data-as="p"><code>delete</code> is the same as <code>unlink</code>.</span> <h2 id="error-codes"><span>Error codes</span></h2> <span data-as="p">When Bun’s S3 API throws an error, it will have a <code>code</code> property that matches one of the following values:</span> <ul> <li><code>ERR_S3_MISSING_CREDENTIALS</code></li> <li><code>ERR_S3_INVALID_METHOD</code></li> <li><code>ERR_S3_INVALID_PATH</code></li> <li><code>ERR_S3_INVALID_ENDPOINT</code></li> <li><code>ERR_S3_INVALID_SIGNATURE</code></li> <li><code>ERR_S3_INVALID_SESSION_TOKEN</code></li> </ul> <span data-as="p">When the S3 Object Storage service returns an error (that is, not Bun), it will be an <code>S3Error</code> instance (an <code>Error</code> instance with the name <code>"S3Error"</code>).</span> <h2 id="s3client-static-methods"><span><code>S3Client</code> static methods</span></h2> <span data-as="p">The <code>S3Client</code> class provides several static methods for interacting with S3.</span> <h3 id="s3client-write-static"><span><code>S3Client.write</code> (static)</span></h3> <span data-as="p">To write data directly to a path in the bucket, you can use the <code>S3Client.write</code> static method.</span> <pre highlight="[12,15,16,17,18,22,25,26,27,28,29]" numberoflines="29" language="typescript" data-language="typescript">import { S3Client } from "bun";

const credentials = {
  accessKeyId: "your-access-key",
  secretAccessKey: "your-secret-key",
  bucket: "my-bucket",
  // endpoint: "https://s3.us-east-1.amazonaws.com",
  // endpoint: "https://&lt;account-id&gt;.r2.cloudflarestorage.com", // Cloudflare R2
};

// Write string
await S3Client.write("my-file.txt", "Hello World");

// Write JSON with type
await S3Client.write("data.json", JSON.stringify({ hello: "world" }), {
  ...credentials,
  type: "application/json",
});

// Write from fetch
const res = await fetch("https://example.com/data");
await S3Client.write("data.bin", res, credentials);

// Write with ACL
await S3Client.write("public.html", html, {
  ...credentials,
  acl: "public-read",
  type: "text/html",
});
</pre> <span data-as="p">This is equivalent to calling <code>new S3Client(credentials).write("my-file.txt", "Hello World")</code>.</span> <h3 id="s3client-presign-static"><span><code>S3Client.presign</code> (static)</span></h3> <span data-as="p">To generate a presigned URL for an S3 file, you can use the <code>S3Client.presign</code> static method.</span> <pre highlight="[11,12,13,14]" numberoflines="14" language="typescript" data-language="typescript">import { S3Client } from "bun";

const credentials = {
  accessKeyId: "your-access-key",
  secretAccessKey: "your-secret-key",
  bucket: "my-bucket",
  // endpoint: "https://s3.us-east-1.amazonaws.com",
  // endpoint: "https://&lt;account-id&gt;.r2.cloudflarestorage.com", // Cloudflare R2
};

const url = S3Client.presign("my-file.txt", {
  ...credentials,
  expiresIn: 3600,
});
</pre> <span data-as="p">This is equivalent to calling <code>new S3Client(credentials).presign("my-file.txt", { expiresIn: 3600 })</code>.</span> <h3 id="s3client-list-static"><span><code>S3Client.list</code> (static)</span></h3> <span data-as="p">To list some or all (up to 1,000) objects in a bucket, you can use the <code>S3Client.list</code> static method.</span> <pre highlight="[12,15,16,17,18,19,20,24,25,26,27,28,29]" numberoflines="30" language="typescript" data-language="typescript">import { S3Client } from "bun";

const credentials = {
  accessKeyId: "your-access-key",
  secretAccessKey: "your-secret-key",
  bucket: "my-bucket",
  // endpoint: "https://s3.us-east-1.amazonaws.com",
  // endpoint: "https://&lt;account-id&gt;.r2.cloudflarestorage.com", // Cloudflare R2
};

// List (up to) 1000 objects in the bucket
const allObjects = await S3Client.list(null, credentials);

// List (up to) 500 objects under `uploads/` prefix, with owner field for each object
const uploads = await S3Client.list({
  prefix: 'uploads/',
  maxKeys: 500,
  fetchOwner: true,
}, credentials);

// Check if more results are available
if (uploads.isTruncated) {
  // List next batch of objects under `uploads/` prefix
  const moreUploads = await S3Client.list({
    prefix: 'uploads/',
    maxKeys: 500,
    startAfter: uploads.contents!.at(-1).key
    fetchOwner: true,
  }, credentials);
}
</pre> <span data-as="p">This is equivalent to calling <code>new S3Client(credentials).list()</code>.</span> <h3 id="s3client-exists-static"><span><code>S3Client.exists</code> (static)</span></h3> <span data-as="p">To check if an S3 file exists, you can use the <code>S3Client.exists</code> static method.</span> <pre highlight="[11]" numberoflines="11" language="typescript" data-language="typescript">import { S3Client } from "bun";

const credentials = {
  accessKeyId: "your-access-key",
  secretAccessKey: "your-secret-key",
  bucket: "my-bucket",
  // endpoint: "https://s3.us-east-1.amazonaws.com",
  // endpoint: "https://&lt;account-id&gt;.r2.cloudflarestorage.com", // Cloudflare R2
};

const exists = await S3Client.exists("my-file.txt", credentials);
</pre> <span data-as="p">The same method also works on <code>S3File</code> instances.</span> <pre highlight="[7]" numberoflines="7" language="typescript" data-language="typescript">import { s3 } from "bun";

const s3file = s3.file("my-file.txt", {
  // ...credentials,
});

const exists = await s3file.exists();
</pre> <h3 id="s3client-size-static"><span><code>S3Client.size</code> (static)</span></h3> <span data-as="p">To quickly check the size of S3 file without downloading it, you can use the <code>S3Client.size</code> static method.</span> <pre highlight="[11]" numberoflines="11" language="typescript" data-language="typescript">import { S3Client } from "bun";

const credentials = {
  accessKeyId: "your-access-key",
  secretAccessKey: "your-secret-key",
  bucket: "my-bucket",
  // endpoint: "https://s3.us-east-1.amazonaws.com",
  // endpoint: "https://&lt;account-id&gt;.r2.cloudflarestorage.com", // Cloudflare R2
};

const bytes = await S3Client.size("my-file.txt", credentials);
</pre> <span data-as="p">This is equivalent to calling <code>new S3Client(credentials).size("my-file.txt")</code>.</span> <h3 id="s3client-stat-static"><span><code>S3Client.stat</code> (static)</span></h3> <span data-as="p">To get the size, etag, and other metadata of an S3 file, you can use the <code>S3Client.stat</code> static method.</span> <pre numberoflines="11" language="typescript" data-language="typescript">import { S3Client } from "bun";

const credentials = {
  accessKeyId: "your-access-key",
  secretAccessKey: "your-secret-key",
  bucket: "my-bucket",
  // endpoint: "https://s3.us-east-1.amazonaws.com",
  // endpoint: "https://&lt;account-id&gt;.r2.cloudflarestorage.com", // Cloudflare R2
};

const stat = await S3Client.stat("my-file.txt", credentials);
</pre> <pre numberoflines="7" language="text" data-language="typescript">{
  etag: "\"7a30b741503c0b461cc14157e2df4ad8\"",
  lastModified: 2025-01-07T00:19:10.000Z,
  size: 1024,
  type: "text/plain;charset=utf-8",
}
</pre> <h3 id="s3client-delete-static"><span><code>S3Client.delete</code> (static)</span></h3> <span data-as="p">To delete an S3 file, you can use the <code>S3Client.delete</code> static method.</span> <pre highlight="[10,15]" numberoflines="15" language="typescript" data-language="typescript">import { S3Client } from "bun";

const credentials = {
  accessKeyId: "your-access-key",
  secretAccessKey: "your-secret-key",
  bucket: "my-bucket",
  // endpoint: "https://s3.us-east-1.amazonaws.com",
};

await S3Client.delete("my-file.txt", credentials);
// equivalent to
// await new S3Client(credentials).delete("my-file.txt");

// S3Client.unlink is alias of S3Client.delete
await S3Client.unlink("my-file.txt", credentials);
</pre> <h2 id="s3://-protocol"><span><code>s3://</code> protocol</span></h2> <span data-as="p">To make it easier to use the same code for local files and S3 files, the <code>s3://</code> protocol is supported in <code>fetch</code> and <code>Bun.file()</code>.</span> <pre numberoflines="2" language="typescript" data-language="typescript">const response = await fetch("s3://my-bucket/my-file.txt");
const file = Bun.file("s3://my-bucket/my-file.txt");
</pre> <span data-as="p">You can additionally pass <code>s3</code> options to the <code>fetch</code> and <code>Bun.file</code> functions.</span> <pre highlight="[2,3,4,5,6]" numberoflines="10" language="typescript" data-language="typescript">const response = await fetch("s3://my-bucket/my-file.txt", {
  s3: {
    accessKeyId: "your-access-key",
    secretAccessKey: "your-secret-key",
    endpoint: "https://s3.us-east-1.amazonaws.com",
  },
  headers: {
    range: "bytes=0-1023",
  },
});
</pre> <h3 id="utf-8,-utf-16,-and-bom-byte-order-mark"><span>UTF-8, UTF-16, and BOM (byte order mark)</span></h3> <span data-as="p">Like <code>Response</code> and <code>Blob</code>, <code>S3File</code> assumes UTF-8 encoding by default.</span> <span data-as="p">When calling one of the <code>text()</code> or <code>json()</code> methods on an <code>S3File</code>:</span> <ul> <li>When a UTF-16 byte order mark (BOM) is detected, it will be treated as UTF-16. JavaScriptCore natively supports UTF-16, so it skips the UTF-8 transcoding process (and strips the BOM). This is mostly good, but it does mean if you have invalid surrogate pairs characters in your UTF-16 string, they will be passed through to JavaScriptCore (same as source code).</li> <li>When a UTF-8 BOM is detected, it gets stripped before the string is passed to JavaScriptCore and invalid UTF-8 codepoints are replaced with the Unicode replacement character (<code>\uFFFD</code>).</li> <li>UTF-32 is not supported.</li> </ul>
</div><div class="_attribution">
  <p class="_attribution-p">
    &copy; bun.com, oven-sh, Jarred Sumner<br>Licensed under the MIT License.<br>
    <a href="https://bun.com/docs/runtime/s3" class="_attribution-link">https://bun.com/docs/runtime/s3</a>
  </p>
</div>
