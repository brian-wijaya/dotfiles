<h1 class="devsite-page-title" tabindex="-1"> Module: tf.keras.activations </h1> <devsite-feature-tooltip ack-key="AckCollectionsBookmarkTooltipDismiss" analytics-category="Site-Wide Custom Events" analytics-action-show="Callout Profile displayed" analytics-action-close="Callout Profile dismissed" analytics-label="Create Collection Callout" class="devsite-page-bookmark-tooltip nocontent" dismiss-button="true" id="devsite-collections-dropdown" dismiss-button-text="Dismiss" close-button-text="Got it">    </devsite-feature-tooltip> <div class="devsite-page-title-meta"><devsite-view-release-notes></devsite-view-release-notes></div>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.keras.activations"> <meta itemprop="path" content="Stable"> </div>   <p>DO NOT EDIT.</p> <p>This file was autogenerated. Do not edit it by hand, since your modifications would be overwritten.</p> <h2 id="functions" data-text="Functions" tabindex="-1">Functions</h2> <p><a href="activations/deserialize"><code translate="no" dir="ltr">deserialize(...)</code></a>: Return a Keras activation function via its config.</p> <p><a href="activations/elu"><code translate="no" dir="ltr">elu(...)</code></a>: Exponential Linear Unit.</p> <p><a href="activations/exponential"><code translate="no" dir="ltr">exponential(...)</code></a>: Exponential activation function.</p> <p><a href="activations/gelu"><code translate="no" dir="ltr">gelu(...)</code></a>: Gaussian error linear unit (GELU) activation function.</p> <p><a href="activations/get"><code translate="no" dir="ltr">get(...)</code></a>: Retrieve a Keras activation function via an identifier.</p> <p><a href="activations/hard_sigmoid"><code translate="no" dir="ltr">hard_sigmoid(...)</code></a>: Hard sigmoid activation function.</p> <p><a href="activations/hard_silu"><code translate="no" dir="ltr">hard_silu(...)</code></a>: Hard SiLU activation function, also known as Hard Swish.</p> <p><a href="activations/hard_silu"><code translate="no" dir="ltr">hard_swish(...)</code></a>: Hard SiLU activation function, also known as Hard Swish.</p> <p><a href="activations/leaky_relu"><code translate="no" dir="ltr">leaky_relu(...)</code></a>: Leaky relu activation function.</p> <p><a href="activations/linear"><code translate="no" dir="ltr">linear(...)</code></a>: Linear activation function (pass-through).</p> <p><a href="activations/log_softmax"><code translate="no" dir="ltr">log_softmax(...)</code></a>: Log-Softmax activation function.</p> <p><a href="activations/mish"><code translate="no" dir="ltr">mish(...)</code></a>: Mish activation function.</p> <p><a href="activations/relu"><code translate="no" dir="ltr">relu(...)</code></a>: Applies the rectified linear unit activation function.</p> <p><a href="activations/relu6"><code translate="no" dir="ltr">relu6(...)</code></a>: Relu6 activation function.</p> <p><a href="activations/selu"><code translate="no" dir="ltr">selu(...)</code></a>: Scaled Exponential Linear Unit (SELU).</p> <p><a href="activations/serialize"><code translate="no" dir="ltr">serialize(...)</code></a></p> <p><a href="activations/sigmoid"><code translate="no" dir="ltr">sigmoid(...)</code></a>: Sigmoid activation function.</p> <p><a href="activations/silu"><code translate="no" dir="ltr">silu(...)</code></a>: Swish (or Silu) activation function.</p> <p><a href="activations/softmax"><code translate="no" dir="ltr">softmax(...)</code></a>: Softmax converts a vector of values to a probability distribution.</p> <p><a href="activations/softplus"><code translate="no" dir="ltr">softplus(...)</code></a>: Softplus activation function.</p> <p><a href="activations/softsign"><code translate="no" dir="ltr">softsign(...)</code></a>: Softsign activation function.</p> <p><a href="activations/silu"><code translate="no" dir="ltr">swish(...)</code></a>: Swish (or Silu) activation function.</p> <p><a href="activations/tanh"><code translate="no" dir="ltr">tanh(...)</code></a>: Hyperbolic tangent activation function.</p>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2022 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 4.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/activations" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/keras/activations</a>
  </p>
</div>
